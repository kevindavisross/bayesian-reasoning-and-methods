# Introduction to Continuous Prior and Posterior Distributions {#continuous}













Bayesian analysis is based on the posterior distribution of parameters $\theta$ given data $y$.
The data $y$ might be discrete (e.g., count data) or continuous (e.g., measurement data).
However, *parameters* $\theta$ almost always take values on a *continuous* scale, even when the data are discrete.
For example, in a Binomial situation, the number of successes $y$ takes values on a discrete scale, but the probability of success on any single trial $\theta$ can potentially take any value in the continuous interval (0, 1).

Recall that the posterior distribution is proportional to the product of the prior distribution and the likelihood.
Thus, there are two probability distributions which will influence the posterior distribution.

- The (unconditional) prior distribution of parameters $\theta$, which is (almost always) a continuous distribution
- The conditional distribution of the data $y$ given parameters $\theta$, which determines the likelihood function. Viewed as a conditional distribution of $y$ given $\theta$, the distribution can be discrete or continuous, corresponding to the data type of $y$. However, the likelihood function treats the data $y$ as fixed and the parameters $\theta$ as varying, and therefore the likelihood function is (almost always) a function of continuous $\theta$.


This section provides an introduction to using continuous prior and posterior distributions to quantify uncertainty about parameters.  Some general notation:

- $\theta$ represents^[$\theta$ is used to denote both: (1) the actual parameter (i.e., the random variable) $\theta$ itself, and (2) possible values of $\theta$.] parameters of interest usually taking values on a continuous scale
- $y$ denotes observed sample data (discrete or continuous)
- $\pi(\theta)$ denotes the prior distribution of $\theta$, usually a probability density function (pdf) over possible values of $\theta$
- $f(y|\theta)$ denotes the likelihood function, a function of continuous $\theta$ for fixed $y$
- $\pi(\theta |y)$ denotes the posterior distribution of $\theta$, the conditional distribution of $\theta$ given the data $y$.



Bayes rule works analogously for a continuous parameter $\theta$, given data $y$
\begin{align*}
\pi(\theta|y) & = \frac{f(y|\theta)\pi(\theta)}{f_Y(y)}\\
&  \\
\pi(\theta|y)  & \propto f(y|\theta)\pi(\theta)\\
\text{posterior} & \propto \text{likelihood}\times \text{prior}
\end{align*}

The continuous analog of the law of total probability is
\[
f_Y(y) = \int_{-\infty}^{\infty}f(y|\theta)\pi(\theta) d\theta
\]



## A brief review of continuous distributions


This section provides a brief review of continuous probability distributions.
Throughout, $U$ represents a continuous random variable that takes values denoted $u$.
In a Bayesian framework, $u$ can represent either values of parameters $\theta$ or values of data $y$.

The probability distribution of a *continuous* random variable is (usually) specified by its
**probability density function (pdf)** (a.k.a., density), usually denoted $f$ or $f_U$.
A pdf $f$ must satisfy:
\begin{align*}
f(u) &\ge 0 \qquad \text{for all } u\\
\int_{-\infty}^\infty f(u) du & = 1
\end{align*}
For a continuous random variable $U$ with pdf $f$ the probability that the random variable falls between any two values $a$ and $b$ is given by the
*area* under the density between those two values.
\[
P(a \le U \le b) =\int_a^b f(u) du
\]
A pdf will assign zero probability to intervals where the density is 0. A pdf is usually defined for all real values, but is often nonzero only for some subset of values, the possible values of the random variable. Given a specific pdf, the generic bounds  $(-\infty, \infty)$
should be replaced by the range of possible values, that is, those values $u$ for which $f(u)>0$.

For example, if $U$ can only take positive values we can write its pdf as
\[
f(u) =
\begin{cases}
\text{some function of $u$}, & u>0,\\
0, & \text{otherwise}
\end{cases}
\]
The "0 otherwise" part is often omitted, but be sure to specify the range of values where $f$ is positive.


The expected value of a continuous random variable $U$ with pdf $f$ is
\[
E(U) = \int_{-\infty}^\infty u\, f(u)\, du
\]

**The probability that a continuous random variable $U$ equals any particular value is 0**: $P(U=u)=0$ for all $u$.
A continuous random variable can take uncountably many distinct values, e.g. $0.500000000\ldots$ is different than $0.50000000010\ldots$ is different than $0.500000000000001\ldots$, etc.
Simulating values of a continuous random variable corresponds to an idealized spinner with an infinitely precise needle which can land on any value in a continuous scale.

A density is an idealized mathematical model for the entire population distribution of infinitely many distinct values of the random variable.
In practical applications, there is some acceptable degree of precision, and events like "X, rounded to 4 decimal places, equals 0.5" correspond to intervals that do have positive probability.
For continuous random variables, it doesn't really make sense to talk about the probability that the random value equals a particular value.
However, we can consider the probability that a random variable is *close to* a particular value.


The density $f(u)$ at value $u$ is *not* a probability
But the density $f(u)$ at value $u$ is related to the probability that the random variable $U$ takes a value 
"close to $u$" in the following sense
\[
P\left(u-\frac{\epsilon}{2} \le U \le u+\frac{\epsilon}{2}\right) \approx f(u)\epsilon, \qquad \text{for small $\epsilon$}
\]
So a random variable $U$ is more likely to take values close to those with greater density.

In general, a pdf is often defined only up to some multiplicative constant $c$, for example
\begin{align*}
f(u) & = c\times\text{some function of $u$}, \quad \text{or}\\
f(u) & \propto \text{some function of $u$}
\end{align*}

The constant  $c$ does not affect the shape of the density as a function of $u$, only the scale on the density (vertical) axis. The absolute scaling on the density axis is somewhat irrelevant; it is whatever it needs to be to provide the proper area. In particular, the total area under the pdf must be 1. The scaling constant is determined by the requirement that  $\int_{-\infty}^\infty f(u)du = 1$. (Remember to replace the generic $(-\infty, \infty)$ bounds with the range of possible values.)

What is important about the pdf is *relative* height. For example, if two values $u$ and $\tilde{u}$ satisfy $f(\tilde{u}) = 2f(u)$ then $U$ is roughly "twice as likely to be near $\tilde{u}$ than $u$"
\[
2 = \frac{f(\tilde{u})}{f(u)} = \frac{f(\tilde{u})\epsilon}{f(u)\epsilon} \approx  \frac{P\left(\tilde{u}-\frac{\epsilon}{2} \le U \le \tilde{u}+\frac{\epsilon}{2}\right)}{P\left(u-\frac{\epsilon}{2} \le U \le u+\frac{\epsilon}{2}\right)}
\]

(ref:cap-exponential-pdf-area) Illustration of $P(1<U<2.5)$ (left) and $P(0.995<U<1.005)$ and $P(1.695<U<1.705)$ (right) for $U$ with an Exponential(1) distribution, with pdf $f_U(u) = e^{-u}, u>0$. The plot on the left displays the true area under the curve over (1, 2.5). The plot on the right illustrates how the probability that $U$ is "close to" $u$ can be approximated by the area of a rectangle with height equal to the density at $u$, $f_U(u)$.  The density height at $u=1$ is twice as large than the density height at $u=1.7$, so the probability that $U$ is "close to" 1 is (roughly) twice as large as the probability that $U$ is "close to" 1.7.

```{r exponential-pdf-area, echo=FALSE, fig.cap="(ref:cap-exponential-pdf-area)", out.width='50%', fig.show='hold'}

x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  geom_ribbon(data=subset(xddf ,x>1 & x<2.5),aes(ymax=y),ymin=0,
              fill="orange",colour=NA)+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(u)) +
  ylab(expression(f[U](u))) #+
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)"))


x<-seq(0, 5, 0.001)
y<-dexp(x,1)
xddf <- data.frame(x=x,y=y)
qplot(x,y,data=xddf,geom="line")+
  scale_y_continuous(limits=c(0, 1), expand=c(0, 0)) +
  theme_classic() +
  xlab(expression(u)) +
  ylab(expression(f[U](u))) +
#  ggtitle(paste("P(1<X<2.5) for X ~ Exponential(1)")) +
  geom_rect(data=xddf, mapping=aes(xmin=1-0.1/2, xmax=1+0.1/2,
                                   ymin=0, ymax=dexp(1, 1)),
            color="orange", fill = "orange") +
    geom_rect(data=xddf, mapping=aes(xmin=1.7-0.1/2, xmax=1.7+0.1/2,
                                   ymin=0, ymax=dexp(1.7, 1)),
            color="skyblue", fill = "skyblue") 

```

A sample of values of a continuous random variable is often displayed in a **histogram** which displays the frequencies of values falling in interval "bins". The vertical axis of a histogram is typically on the density scale, so that *areas* of the bars correspond to relative frequencies.

```{python, echo = FALSE}

RV(Exponential(1)).sim(10000).plot()
Exponential(1).plot()

plt.show()

```



## Continuous distributions for a population proportion


We have seen a few examples where we used Normal distributions as prior distributions for a population proportion $\theta$.
Normal distributions are commonly used as priors, but they do not allow for asymmetric prior distributions.
We'll now consider Beta distributions, a family of distributions that are commonly used as prior distributions for population proportions.

```{example, data-singular-beta1}
Continuing Example \@ref(exm:data-singular) where
$\theta$ represents the population proportion of students in Cal Poly statistics classes who prefer to consider data as a singular noun.

```

1. Assume a continuous prior distribution for $\theta$ which is proportional to $\theta^2,\; 0<\theta<1$.  Sketch this distribution.
1. The previous part implies that $\pi(\theta)=c \theta^2,\; 0<\theta<1$, for an appropriate constant $c$.  Find $c$.
1. Compute the prior mean of $\theta$.
1. Now we'll consider a few more prior distributions.  Sketch each of the following priors.  How do they compare?
    a. proportional to $\theta^2,\; 0<\theta<1$. (from previous)
    a. proportional to $\theta^5,\; 0<\theta<1$. 
    a. proportional to $(1-\theta)^2,\; 0<\theta<1$. 
    a. proportional to $\theta^2(1-\theta)^2,\; 0<\theta<1$. 
    a. proportional to $\theta^5(1-\theta)^2,\; 0<\theta<1$. 
   
   

```{solution}
to Example \@ref(exm:data-singular-beta1)
```

1. See the plot below.  The distribution is similar to the discrete grid approximation in  Example \@ref(exm:data-singular2).
1. Set the total area under the curve equal to 1 and solve for $c=3$
\[
1 = \int_0^1 c\theta^2 d\theta = c \int_0^1 \theta^2d\theta = c (1/3) \Rightarrow c = 3
\]
1. Since $\theta$ is continuous we use calculus
\[
E(\theta) = \int_0^1 \theta \,\pi(\theta)d\theta = \int_0^1 \theta (3\theta^2)d\theta = 3/4
\]
1. See the plot below.  The prior proportional to $(1-\theta)^2$ is the mirror image of the prior proportional to $\theta^2$, reflected about 0.5.  As the exponent on $\theta$ increases, more density is shifted towards 1.  As the exponent on $1-\theta$ increases, more density is shifted towards 0.  When the exponents are the same, the density is symmetric about 0.5

```{python, echo = FALSE}

Beta(3, 1).plot()
Beta(6, 1).plot()
Beta(1, 3).plot()
Beta(3, 3).plot()
Beta(6, 3).plot()

plt.legend([r"$\theta^2$", r"$\theta^5$", r"$(1-\theta)^2$", r"$\theta^2(1-\theta)^2$", r"$\theta^5(1-\theta)^2$"]);
plt.show()

```


A continuous random variable $U$ has a **Beta distribution** with *shape parameters* $\alpha>0$ and $\beta>0$ if its density satisfies^[The expression defines the shape of the Beta density.  All that's  missing is the scaling constant which ensures that the total area under the density is 1.  The actual Beta density formula, including the normalizing constant, is
	\[
	f(u) =\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\; u^{\alpha-1}(1-u)^{\beta-1}, \quad 0<u<1,
	\]
	where 
	$\Gamma(\alpha) = \int_0^\infty e^{-v}v^{\alpha-1} dv$ is the *Gamma function*. For a positive integer $k$,
	$\Gamma(k) = (k-1)!$.  Also, $\Gamma(1/2)=\sqrt{\pi}$.
]
\[
f(u) \propto u^{\alpha-1}(1-u)^{\beta-1}, \quad 0<u<1,
\]
and $f(u)=0$ otherwise.

- If $\alpha = \beta$ the distribution is symmetric about 0.5
- If $\alpha > \beta$ the distribution is skewed to the left (with greater density above 0.5 than below)
- If $\alpha < \beta$ the distribution is skewed to the right (with greater density below 0.5 than above)
- If $\alpha = 1$ and $\beta = 1$, the Beta(1, 1) distribution is the Uniform distribution on (0, 1).

It can be shown that a Beta($\alpha$, $\beta$) density has 
\begin{align*}
\text{Mean (EV):} & \quad \frac{\alpha}{\alpha+\beta}\\
\text{Variance:} & \quad \frac{\left(\frac{\alpha}{\alpha+\beta}\right)\left(1-\frac{\alpha}{\alpha+\beta}\right)}{\alpha+\beta+1}
\\
\text{Mode:} & \quad \frac{\alpha -1}{\alpha+\beta-2}, \qquad \text{(if $\alpha>1$, $\beta\ge1$ or $\alpha\ge1$, $\beta>1$)}
\end{align*}

```{example, data-singular-beta2}
Continuing  Example \@ref(exm:data-singular-beta1)
```


1. Each of the previous distributions in the previous example was a Beta distribution.  For each distribution, identify the shape parameters and the prior mean and standard deviation.
    a. proportional to $\theta^2,\; 0<\theta<1$.
    a. proportional to $\theta^5,\; 0<\theta<1$. 
    a. proportional to $(1-\theta)^2,\; 0<\theta<1$. 
    a. proportional to $\theta^2(1-\theta)^2,\; 0<\theta<1$. 
    a. proportional to $\theta^5(1-\theta)^2,\; 0<\theta<1$. 
1. Now suppose that 31 students in a sample of 35 Cal Poly statistics students prefer data as singular. Specify the shape of the likelihood as a function of $\theta, 0<\theta<1$.
1. Starting with each of the prior distributions from the first part, find the posterior distribution of $\theta$ based on this sample, and identify it as a Beta distribution by specifying the shape parameters $\alpha$ and $\beta$
    a. proportional to $\theta^2,\; 0<\theta<1$.
    a. proportional to $\theta^5,\; 0<\theta<1$. 
    a. proportional to $(1-\theta)^2,\; 0<\theta<1$. 
    a. proportional to $\theta^2(1-\theta)^2,\; 0<\theta<1$. 
    a. proportional to $\theta^5(1-\theta)^2,\; 0<\theta<1$.
1. For each of the posterior distributions in the previous part, compute the posterior mean and standard deviation.  How does each posterior distribution compare to its respective prior distribution?


```{solution}
to Example \@ref(exm:data-singular-beta2)
```

1. Careful with the exponents.  For example, $\theta^2 = \theta^2(1-\theta)^0 = \theta^{3-1}(1-\theta)^{1-1}$, which corresponds to a Beta(3, 1) distribution.

    |   | Distribution | $\alpha$ | $\beta$ |                      Proportional to |  Mean |    SD |
    |---|--------------|---------:|--------:|-------------------------------------:|------:|------:|
    | a | Beta(3, 1)   |        3 |       1 |             $\theta^2, 0<\theta < 1$ | 0.750 | 0.194 |
    | b | Beta(6, 1)   |        6 |       1 |             $\theta^5, 0<\theta < 1$ | 0.857 | 0.124 |
    | c | Beta(1, 3)   |        1 |       3 |         $(1-\theta)^2, 0<\theta < 1$ | 0.250 | 0.194 |
    | d | Beta(3, 3)   |        3 |       3 | $\theta^2(1-\theta)^2, 0<\theta < 1$ | 0.500 | 0.189 |
    | e | Beta(6, 3)   |        6 |       3 | $\theta^5(1-\theta)^2, 0<\theta < 1$ | 0.667 | 0.149 |


1. Given $\theta$, the number of students in the sample who prefer data as singular, $Y$, follows a Binomial(35, $\theta$) distribution.  The likelihood is the probability of observing $Y=31$ viewed as a function of $\theta$.
\begin{align*}
f(31|\theta) & =  \binom{35}{31}\theta^{31}(1-\theta)^4, \qquad 0 < \theta <1\\
& \propto \theta^{31}(1-\theta)^4, \qquad 0 < \theta <1
\end{align*}
The constant $\binom{35}{31}$ does not affect the *shape* of the likelihood as a function of $\theta$.
1. As always, the posterior distribution is proportional to the product of the prior distribution and the likelihood.  For the Beta(3, 1) prior, the prior density is proportional to $\theta^2$, $0<\theta<1$, and for the observed data $y=31$ with $n=35$, the likelihood is proportional to $\theta^{31}(1-\theta)^4$, $0<\theta<1$.  Therefore, the posterior density, as a function of $\theta$, is proportional to
\begin{align*}
\pi(\theta|y = 31) & \propto \left(\theta^2\right)\left(\theta^{31}(1-\theta)^4\right), \qquad 0 <\theta < 1\\
& \propto \theta^{33}(1-\theta)^4, \qquad 0 <\theta < 1\\
& \propto \theta^{34 - 1}(1-\theta)^{5 - 1}, \qquad 0 <\theta < 1\\
\end{align*}
    Therefore, the posterior distribution of $\theta$ is the Beta(3 + 31, 1 + 35 - 31), that is, the Beta(34, 5) distribution.
    The other situations are similar.  The prior changes but the likelihood stays the same, based on a sample with 31 successes and $35-31 = 4$ failures.  If the prior distribution is Beta($\alpha$, $\beta$) then the posterior distribution  is Beta($\alpha + 31$, $\beta + 35 - 31$).
    
    |   | Prior Distribution |                           Posterior proportional to | Posterior Distribution | Posterior Mean | Posterior SD |
    |---|--------------------|----------------------------------------------------:|-----------------------:|---------------:|-------------:|
    | a | Beta(3, 1)         | $\theta^{2 + 31}(1-\theta)^{0 + 4}, 0 < \theta < 1$ |            Beta(34, 5) |          0.872 |        0.053 |
    | b | Beta(6, 1)         | $\theta^{5 + 31}(1-\theta)^{0 + 4}, 0 < \theta < 1$ |            Beta(37, 5) |          0.881 |        0.049 |
    | c | Beta(1, 3)         | $\theta^{0 + 31}(1-\theta)^{2 + 4}, 0 < \theta < 1$ |            Beta(32, 7) |          0.821 |        0.061 |
    | d | Beta(3, 3)         | $\theta^{2 + 31}(1-\theta)^{2 + 4}, 0 < \theta < 1$ |            Beta(34, 7) |          0.829 |        0.058 |
    | e | Beta(6, 3)         | $\theta^{5 + 31}(1-\theta)^{2 + 4}, 0 < \theta < 1$ |            Beta(37, 7) |          0.841 |        0.055 |

    <!-- |   | Prior Distribution |                Prior Proportional to |      Likelihood of $y=31$ proportional to |                   Posterior proportional to | Posterior Distribution | Posterior Mean | Posterior SD | -->
    <!-- |---|--------------------|-------------------------------------:|------------------------------------------:|--------------------------------------------:|-----------------------:|---------------:|-------------:| -->
    <!-- | a | Beta(3, 1)         |             $\theta^2, 0<\theta < 1$ | $\theta^{31}(1-\theta)^4, 0 < \theta < 1$ | $\theta^{33}(1-\theta)^{4}, 0 < \theta < 1$ |            Beta(34, 5) |          0.872 |        0.053 | -->
    <!-- | b | Beta(6, 1)         |             $\theta^5, 0<\theta < 1$ | $\theta^{31}(1-\theta)^4, 0 < \theta < 1$ |   $\theta^{36}(1-\theta)^4, 0 < \theta < 1$ |            Beta(37, 5) |          0.881 |        0.049 | -->
    <!-- | c | Beta(1, 3)         |         $(1-\theta)^2, 0<\theta < 1$ | $\theta^{31}(1-\theta)^4, 0 < \theta < 1$ |   $\theta^{31}(1-\theta)^6, 0 < \theta < 1$ |            Beta(32, 7) |          0.821 |        0.061 | -->
    <!-- | d | Beta(3, 3)         | $\theta^2(1-\theta)^2, 0<\theta < 1$ | $\theta^{31}(1-\theta)^4, 0 < \theta < 1$ |   $\theta^{33}(1-\theta)^6, 0 < \theta < 1$ |            Beta(34, 7) |          0.829 |        0.058 | -->
    <!-- | e | Beta(6, 3)         | $\theta^5(1-\theta)^2, 0<\theta < 1$ | $\theta^{31}(1-\theta)^4, 0 < \theta < 1$ |   $\theta^{36}(1-\theta)^6, 0 < \theta < 1$ |            Beta(37, 7) |          0.841 |        0.055 | -->
    
1. See the table above.  Each posterior distribution concentrates more probability towards the observed sample proportion $31/35 = 0.886$, though there are some small differences due to the prior.  The posterior SD is less than the prior SD; there is less uncertainty about $\theta$ after observing some data.


```{python, echo = FALSE, message = FALSE, warning = FALSE, fig.show="hold", out.width="50%"}

plt.figure()

Beta(3, 1).plot()
Beta(6, 1).plot()
Beta(1, 3).plot()
Beta(3, 3).plot()
Beta(6, 3).plot()

plt.legend([r"Beta(3, 1)", r"Beta(6, 1)", r"Beta(1, 3)", r"Beta(3, 3)", r"Beta(6, 3)"]);
plt.show();

n = 35
y = 31

plt.figure()

Beta(3 + y, 1 + n - y).plot()
Beta(6 + y, 1 + n - y).plot()
Beta(1 + y, 3 + n - y).plot()
Beta(3 + y, 3 + n - y).plot()
Beta(6 + y, 3 + n - y).plot()

plt.legend([r"Beta(34, 5)", r"Beta(37, 5)", r"Beta(32, 7)", r"Beta(34, 7)", r"Beta(37, 7)"]);
plt.show();


```



Beta distributions are often used in Bayesian models involving population proportions.
Consider some binary ("success/failure") variable and let $\theta$ be the population proportion of success.
Select a random sample of size $n$ from the population and let $Y$ count the number of successes in the sample.


**Beta-Binomial model.**  If $\theta$ has a Beta$(\alpha, \beta)$ prior distribution and the conditional distribution of $Y$ given $\theta$ is the Binomial$(n, \theta)$ distribution, then the posterior distribution of $\theta$ given $y$ is the Beta$(\alpha + y, \beta + n - y)$ distribution.
\begin{align*}
\text{prior:} & & \pi(\theta) & \propto \theta^{\alpha-1}(1-\theta)^{\beta-1}, \quad 0<\theta<1,\\
\\
\text{likelihood:} & & f(y|\theta) & \propto \theta^y (1-\theta)^{n-y}, \quad 0 < \theta < 1,\\
\\
\text{posterior:} & & \pi(\theta|y) & \propto \theta^{\alpha+y-1}(1-\theta)^{\beta+n-y-1}, \quad 0<\theta<1.
\end{align*}

Try this [applet which illustrates the Beta-Binomial model](https://shiny.stat.ncsu.edu/bjreich/BetaBinom/).

In a sense, you can interpret $\alpha$ as "prior successes" and $\beta$ as "prior failures", but these are only "pseudo-observations".  Also, $\alpha$ and $\beta$ are not necessarily integers.


|           |          Prior |  Data |            Posterior |
|-----------|---------------:|------:|---------------------:|
| Successes |       $\alpha$ |   $y$ |         $\alpha + y$ |
| Failures  |        $\beta$ | $n-y$ |      $\beta + n - y$ |
| Total     | $\alpha+\beta$ |   $n$ | $\alpha + \beta + n$ |



When the prior and posterior distribution belong to the same family, that family is called a **conjugate** prior distribution for the likelihood. So, the Beta distributions form a conjugate prior family for Binomial distributions.




```{example, data-singular-beta3}
In Example \@ref(exm:data-singular2) we used a grid approximation to the prior distribution of $\theta$.
Now we will assume a continuous prior distributions.  Assume that $\theta$ has a Beta(3, 1) prior distribution and that 31 students in a sample of 35 Cal Poly statistics students prefer data as singular. 

```

1. Plot the prior distribution, (scaled) likelihood, and posterior distribution.
1. Use software to find 50%, 80%, and 98% central posterior credible intervals.
1. Compare the results to those using the grid approximation in Example \@ref(exm:data-singular2).
1. Express the posterior mean as a weighted average of the prior mean and sample proportion.  Describe what the weights are, and explain why they make sense.



```{solution}
to Example \@ref(exm:data-singular-beta3)
```

1. See plot below.  The posterior distribution is the Beta(34, 5) distribution.
Note that the grid in the code is just to plot things in R.
In particular, the posterior is computed using the Beta-Binomial model, not the grid.

    ```{r}

theta = seq(0, 1, 0.0001) # the grid is just for plotting

# prior
alpha_prior = 3
beta_prior = 1
prior = dbeta(theta, alpha_prior, beta_prior)

# data
n = 35
y = 31

# likelihood
likelihood = dbinom(y, n, theta)

# posterior
alpha_post = alpha_prior + y
beta_post = beta_prior + n - y
posterior = dbeta(theta, alpha_post, beta_post)

# plot
ymax = max(c(prior, posterior))
scaled_likelihood = likelihood * ymax / max(likelihood)

plot(theta, prior, type='l', col='skyblue', xlim=c(0, 1), ylim=c(0, ymax), ylab='', yaxt='n')
par(new=T)
plot(theta, scaled_likelihood, type='l', col='orange', xlim=c(0, 1), ylim=c(0, ymax), ylab='',  yaxt='n')
par(new=T)
plot(theta, posterior, type='l', col='seagreen', xlim=c(0, 1), ylim=c(0, ymax), ylab='', yaxt='n')
legend("topleft", c("prior", "scaled likelihood", "posterior"), lty=1, col=c("skyblue", "orange", "seagreen"))

# 50% posterior credible interval
qbeta(c(0.25, 0.75), alpha_post, beta_post)

# 80% posterior credible interval
qbeta(c(0.1, 0.9), alpha_post, beta_post)

# 98% posterior credible interval
qbeta(c(0.01, 0.99), alpha_post, beta_post)

    ```

1. We can use `qbeta` to compute quantiles (a.k.a. percentiles). The posterior mean is 0.872, and the prior standard deviation is 0.053. There is a posterior probability of 50% that between 84.0% and 91.1% of Cal Poly students prefer data as singular; after observing the sample data, it's equally plausible that $\theta$ is inside [0.840, 0.911] as outside.
There is a posterior probability of 80% that between 80.0% and 93.5% of Cal Poly students prefer data as singular; after observing the sample data, it's four times mores plausible that $\theta$ is inside [0.800, 0.935] as outside.
There is a posterior probability of 98% that between 72.4% and 96.5% of Cal Poly students prefer data as singular; after observing the sample data, it's 49 times mores plausible that $\theta$ is inside [0.724, 0.965] as outside.
1. The results based on continuous distributions are the same as those for the grid approximation.  The grid is just an approximation of the "true" Beta-Binomial theory.
1. The prior mean is $\frac{3}{3+1}=0.75$.  The sample proportion is $\frac{31}{35} = 0.886$.  The posterior mean is $\frac{34}{39} = 0.872$.  We can write
\begin{align*}
\frac{34}{39} & = \left(\frac{3}{4}\right)\times \left(\frac{4}{39}\right) + \left(\frac{31}{35}\right)\times \left(\frac{35}{39}\right)\\
 & = \left(\frac{3}{4}\right)\times \left(\frac{4}{4 + 35}\right) + \left(\frac{31}{35}\right)\times \left(\frac{35}{4 + 35}\right)\\
\end{align*}
    The posterior mean is a weighted average of the prior mean and the sample proportion where the weights are given by the relative "samples sizes".  The "prior sample size" is $3+1=4$.  The actual observed sample size is 35.  
    
    
In the Beta-Binomial model, the posterior mean $E(\theta|y)$ can be expressed as a *weighted average* of the prior mean $E(\theta)=\frac{\alpha}{\alpha + \beta}$ and the sample proportion $\hat{p}=y/n$.
\[
E(\theta|y) = \frac{\alpha+\beta}{\alpha+\beta+n}E(\theta) + \frac{n}{\alpha+\beta+n}\hat{p}
\]
As more data are collected, more weight is given to the sample proportion (and less weight to the prior mean).
The prior "weight" is detemined by $\alpha+\beta$, which is sometimes called the *concentration* and measured in "pseudo-observations".
Larger values of $\alpha+\beta$ indicate stronger prior beliefs, due to smaller prior variance, and give more weight to the prior mean.


The posterior variance generally gets smaller as more data are collected
\[
\text{Var}(\theta |y) = \frac{E(\theta|y)(1-E(\theta|y))}{\alpha+\beta+n+1}
\]


```{example, data-singular-beta-prediction}
Now let's reconsider the posterior prediction parts of Example \@ref(exm:data-singular2), treating $\theta$ as continuous.
Assume that $\theta$ has a Beta(3, 1) prior distribution and that 31 students in a sample of 35 Cal Poly statistics students prefer data as singular, so that the posterior distribution of $\theta$ is the Beta(34, 5) distribution. 

```

1. Suppose we plan to randomly select another sample of 35 Cal Poly statistics students.  Let $\tilde{Y}$ represent the number of students in the selected sample who prefer data as singular. How could we use simulation to approximate the posterior predictive distribution of $\tilde{Y}$?
1. Use software to run the simulation and plot the posterior predictive distribution^[The posterior predictive distribution can be found analytically in the Beta-Binomial situation.
If $\theta\sim$ Beta$(\alpha, \beta)$ and $(Y|\theta)\sim$ Binomial$(n, \theta)$ then the marginal distribution of $Y$ is the [Beta-Binomial](https://en.wikipedia.org/wiki/Beta-binomial_distribution) distribution with
\[
P(Y = y) = \binom{n}{y}\frac{B(\alpha+y,\beta+n-y)}{B(\alpha, \beta)}, \qquad y = 0, 1, \ldots, n,
\]
$B(\alpha, \beta)$ is the *beta function*, for which $B(\alpha,\beta)=\frac{(\alpha-1)!(\beta-1)!}{(\alpha+\beta-1)!}$ if $\alpha,\beta$ are positive integers. (For general $\alpha,\beta>0$, $B(\alpha,\beta)=\int_0^1u^{\alpha-1} (1-u)^{\beta-1}du = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha+\beta)}$.) The mean is $n\left(\frac{\alpha}{\alpha+\beta}\right)$. In R: `dbbinom, rbbinom, pbbinom` in `extraDistr` package].
Compare to Example \@ref(exm:data-singular2).
1. Use the simulation results to approximate a 95% posterior *prediction* interval for $\tilde{Y}$.  Write a clearly worded sentence interpreting this interval in context.





```{solution}
to Example \@ref(exm:data-singular-beta3)
```


1. Simulate a value of $\theta$ from the posterior Beta(34, 5) distribution.  Given this value of $\theta$, simulate a value $\tilde{y}$ from a Binomial(35, $\theta$) distribution.  Repeat many times, simulating many $(\theta, \tilde{y})$ pairs. The simulated distribution of $\tilde{y}$ values will approximate the posterior predictive distribution.
1. We can use `rbeta` to simulate from a Beta distribution.  The simulation results are similar to those from the grid approximation.

    ```{r}
    n_sim = 10000
    
    theta_sim = rbeta(n_sim, 34, 5)
    
    y_sim = rbinom(n_sim, 35, theta_sim)
    
    plot(table(y_sim) / n_sim, xlab = "y", ylab = "Posterior predictive probability", col =  "#CC79A7")
    
    quantile(y_sim, c(0.025, 0.975))
    

    ```

1. The interval is similar to the one from the grid approximation, and the interpretation is the same.
There is posterior predictive probability of 95% that between 24 and 35 students in a sample of 35 students will prefer data as singular.










You can tune the shape parameters --- $\alpha$ (like "prior successes") and $\beta$ (like "prior failures") --- of a Beta distribution to your prior beliefs in a few ways.
Recall that $\kappa = \alpha + \beta$ is the "concentration" or "equivalent prior sample size".

- If prior mean $\mu$ and prior concentration $\kappa$ are specified then
\begin{align*}
\alpha &= \mu \kappa\\
\beta & =(1-\mu)\kappa
\end{align*}
- If prior mode $\omega$ and prior concentration $\kappa$ (with $\kappa>2$) are specified then
\begin{align*}
\alpha &= \omega (\kappa-2) + 1\\
\beta & = (1-\omega) (\kappa-2) + 1
\end{align*}
- If prior mean $\mu$ and prior sd $\sigma$ are specified then
\begin{align*}
\alpha &= \mu\left(\frac{\mu(1-\mu)}{\sigma^2} -1\right)\\
\beta & = \left(1-\mu\right)\left(\frac{\mu(1-\mu)}{\sigma^2} -1\right)
%\beta & = \alpha\left(\frac{1}{\mu} - 1\right)\\
\end{align*}
- You can also [specify two percentiles and use software to find $\alpha$ and $\beta$](https://bayesball.shinyapps.io/ChooseBetaPrior_3/).
For example, you could specify the endpoints of a prior 98% credible interval.


```{example, choose-beta-prior}
Suppose we want to estimate $\theta$, the proportion of Cal Poly students that are left-handed.
```


1. Sketch your Beta prior distribution for $\theta$.  Describe its main features and your reasoning.
Then translate your prior into a Beta distribution by specifying the shape parameters $\alpha$ and $\beta$.
1. Assume a prior Beta distribution for $\theta$ with prior mean 0.15 and prior SD is 0.08.
Find $\alpha$ and $\beta$, and a prior 98% credible interval for $\theta$.

```{solution}
to Example \@ref(exm:choose-beta-prior)
```

1. Of course, choices will vary, based on what you know about left-handedness.
But do think about what your prior might look like, and use one of the methods to translate it to a Beta distribution.
1. Let's say we've heard that about 15% of people in general are left-handed, but we've also heard 10% so we're not super sure, and we also don't know how Cal Poly students compare to the general population.
So we'll assume a prior Beta distribution for $\theta$ with prior mean 0.15 (our "best guess") and a prior SD of 0.08 to reflect our degree of uncertainty.
This translates to a Beta(2.8, 16.1) prior, with a central 98% prior credible interval for $\theta$ that between 2.2% and 38.1% of Cal Poly students are left-handed.
We could probably go with more prior certainty than this, but it seems at least like a reasonable starting place before observing data.
We can (and should) use prior predictive tuning to aid in choosing $\alpha$ and $\beta$ for our Beta distribution prior.



```{r}

mu = 0.15
sigma = 0.08

alpha = mu ^ 2 * ((1 - mu) / sigma ^ 2 - 1 / mu); alpha
beta <- alpha * (1 / mu - 1); beta

qbeta(c(0.01, 0.99), alpha, beta)

```





<!-- ## Continuous distributions for a population mean -->




