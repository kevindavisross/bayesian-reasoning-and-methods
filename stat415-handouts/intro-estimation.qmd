# Introduction to Estimation

{{< include _r_setup.qmd >}}

{{< include _python_setup.qmd >}}



- A *parameter* is a number that describes the population, e.g., population mean, population proportion. The actual value of a parameter is almost always unknown.
- A *statistic* is a number that describes the sample, e.g., sample mean, sample proportion.
- Observed sample statistics can be used to *estimate* unknown population parameters.
- **Bayesian estimation**
    - Regards parameters as *random variables* with probability distributions
    - Assigns a subjective **prior distribution** to parameters
    - *Conditions* on the observed data
    - Determines parameter estimates based on the **posterior distribution**, which is obtained via Bayes rule
    $$
    \text{posterior} \propto \text{prior} \times \text{likelihood}
    $$
- For given data $y$, the **likelihood function** $f(y|\theta)$ is the probability (or density for continuous data) of observing the sample data $y$ viewed as a *function of the parameter* $\theta$.
    - In the likelihood function, the observed value of the data $y$ is treated as a fixed constant; the likelihood is a function of parameters $\theta$.
    - The likelihood function is used in both Bayesian and frequentist statistics, but in different ways^[For example, maximum likelihood estimation is a common *frequentist* technique for estimating the value of a parameter based on data from a sample. The value of a parameter that maximizes the likelihood function is called a *maximum likelihood estimate* (MLE). The MLE depends on the data $y$: the MLE is the value of $\theta$ which gives the largest likelihood of having produced the observed data $y$.].





::: {#exm-kissing-discrete}
Most people are right-handed, and even the right eye is dominant for most people.
In a [2003 study reported in *Nature*](http://www.nature.com/news/2003/030213/full/news030210-7.html), a German bio-psychologist conjectured that this preference for the right side manifests itself in other ways as well.
In particular, he investigated if people have a tendency to lean their heads to the right when kissing.
The researcher observed kissing couples in public places and recorded whether the couple leaned their heads to the right or left.
(We'll assume this represents a randomly selected representative sample of kissing couples.)
	
The parameter of interest is the *population proportion of kissing couples who lean their heads to the right*.
Denote this unknown parameter $\theta$; our goal is to estimate $\theta$ based on sample data.
We'll take a Bayesian approach to estimating $\theta$: we'll treat the unknown parameter $\theta$ as a *random variable* and wish to find its posterior distribution after observing data on a sample of $n$ kissing couples of which $y$ lean their heads to the right.
:::

1. Sketch a distribution representing your plausibility assessment of $\theta$ prior to observing the sample data.
\
\
\
\
\

1. We will start with a very simplified, unrealistic prior distribution that assumes only five possible, equally likely values for $\theta$: 0.1, 0.3, 0.5, 0.7, 0.9.
Sketch the prior distribution and start constructing a Bayes table.
\
\
\
\
\

1. Now suppose that we observe a sample of $n=12$ kissing couples of which $y=8$ lean right.
Describe in principle would you fill in the likelihood column.
\
\
\
\
\

1. If $\theta=0.1$ what is the distribution of $y$?
Compute and interpret the probability that $y=8$ if $\theta = 0.1$.
\
\
\
\
\

1. If $\theta=0.3$ what is the distribution of $y$?
Compute and interpret the probability that $y=8$ if $\theta = 0.3$.
\
\
\
\
\

1. If $\theta=0.5$ what is the distribution of $y$?
Compute and interpret the probability that $y=8$ if $\theta = 0.5$.
\
\
\
\
\

1. If $\theta=0.7$ what is the distribution of $y$?
Compute and interpret the probability that $y=8$ if $\theta = 0.7$.
\
\
\
\
\

1. If $\theta=0.9$ what is the distribution of $y$?
Compute and interpret the probability that $y=8$ if $\theta = 0.9$.
\
\
\
\
\

1. Sketch a plot of the likelihood function and fill in the likelihood column in the Bayes table.
\
\
\
\
\

1. Complete the Bayes table and sketch a plot of the posterior distribution.
\
\
\
\
\

1. Examine the posterior distribution.
What does it say about $\theta$?
How does it compare to the prior and the likelihood?
\
\
\
\
\

1. Now consider a prior distribution which places probability 1/9, 2/9, 3/9, 2/9, 1/9 on the values 0.1, 0.3, 0.5, 0.7, 0.9, respectively.
Assuming the same sample data ($y/n = 8/12$), construct a Bayes table using this prior.
How does the posterior distribution compare to the one based on the equally likely prior?
\
\
\
\
\

1. Now consider a prior distribution which places probability 5/15, 4/15, 3/15, 2/15, 1/15 on the values 0.1, 0.3, 0.5, 0.7, 0.9, respectively.
Assuming the same sample data ($y/n = 8/12$), construct a Bayes table using this prior.
How does the posterior distribution compare to the ones from the previous parts?
\
\
\
\
\

- In Bayesian estimation parameters are treated as *random variables* with probability distributions^[ In contrast, a frequentist approach regards parameters as unknown but fixed (not random) quantities.].
- The *posterior distribution* contains all relevant information about parameters after observing sample data.
That is, all Bayesian inference is based on the posterior distribution.
- The posterior distribution is a compromise between
    - prior "beliefs/plausibility", as represented by the prior distribution
    - data, as represented by the likelihood function





::: {#exm-kissing-grid}
Continuing @exm-kissing-discrete.
While the previous exercise introduced the main ideas, it was unrealistic to consider only five possible values of $\theta$.  

:::

1. What are the *possible* values of $\theta$?
Does the *parameter* $\theta$ take values on a continuous or discrete scale?
(Careful: we're talking about the parameter and not the data.)
\
\
\
\
\

1. Let's assume that any multiple of 0.0001 is a possible value of $\theta$: $0, 0.0001, 0.0002, \ldots, 0.9999, 1$.
Assume a discrete uniform (equally likely) prior distribution on these values.
Suppose again that $y=8$ couples in a sample of $n=12$ kissing couples lean right.
Use software to plot the prior distribution, the (scaled) likelihood function, and then find the posterior and plot it.
Describe the posterior distribution.
What does it say about $\theta$?
\
\
\
\
\

1. Now assume a prior distribution which is proportional to $1-2|\theta-0.5|$ for $\theta = 0, 0.0001, 0.0002, \ldots, 0.9999, 1$.
Use software to plot this prior; what does it say about $\theta$?
Then suppose again that $y=8$ couples in a sample of $n=12$ kissing couples lean right.
Use software to plot the prior distribution, the (scaled) likelihood function, and then find the posterior and plot it.
What does the posterior distribution say about $\theta$?
\
\
\
\
\

1. Now assume a prior distribution which is proportional to $1-\theta$ for $\theta = 0, 0.0001, 0.0002, \ldots, 0.9999, 1$.
Use software to plot this prior; what does it say about $\theta$?
Then suppose again that $y=8$ couples in a sample of $n=12$ kissing couples lean right.
Use software to plot the prior distribution, the (scaled) likelihood function, and then find the posterior and plot it.
What does the posterior distribution say about $\theta$?
\
\
\
\
\

1. Compare the posterior distributions corresponding to the three different priors.
How does each posterior distribution compare to the prior and the likelihood?
Does the prior distribution influence the posterior distribution?
\
\
\
\
\



- Even in situations where the data are discrete (e.g., binary success/failure data, count data), most statistical *parameters* take values on a *continuous* scale.
- Thus in a Bayesian analysis, parameters are usually *continuous random variables*, and have *continuous probability distributions*, a.k.a., *densities*.
- An alternative to dealing with continuous distributions is to use **grid approximation**: Treat the parameter as discrete, on a sufficiently fine grid of values, and use discrete distributions.




::: {#exm-kissing-data}
Continuing @exm-kissing-grid.
Now we'll perform a Bayesian analysis on the actual study data in which $y=80$ couples out of a sample of $n=124$ leaned right.
We'll again use a grid approximation and assume that any multiple of 0.0001 between 0 and 1 is a possible value of $\theta$: $0, 0.0001, 0.0002, \ldots, 0.9999, 1$.

:::

1. Before performing the Bayesian analysis, use software to plot the likelihood when $y=80$ couples in a sample of $n=124$ kissing couples lean right.
How does the likelihood for this sample compare to the likelihood based on the smaller sample (8/12) from previous exercises?
\
\
\
\
\

1. Now back to Bayesian analysis.
Assume a discrete uniform prior distribution for $\theta$.
Suppose that $y=80$ couples in a sample of $n=124$ kissing couples lean right.
Use software to plot the prior distribution, the likelihood function, and then find the posterior and plot it.
Describe the posterior distribution.
What does it say about $\theta$?
\
\
\
\
\

1. Now assume a prior distribution which is proportional to $1-2|\theta-0.5|$ for $\theta = 0, 0.0001, 0.0002, \ldots, 0.9999, 1$.
Then suppose again that $y=80$ couples in a sample of $n=124$ kissing couples lean right.
Use software to plot the prior distribution, the likelihood function, and then find the posterior and plot it.
What does the posterior distribution say about $\theta$?
\
\
\
\
\

1. Now assume a prior distribution which is proportional to $1-\theta$ for $\theta = 0, 0.0001, 0.0002, \ldots, 0.9999, 1$.
Then suppose again that $y=80$ couples in a sample of $n=124$ kissing couples lean right.
Use software to plot the prior distribution, the likelihood function, and then find the posterior and plot it.
What does the posterior distribution say about $\theta$?
\
\
\
\
\

1. Compare the posterior distributions corresponding to the three different priors.
How does each posterior distribution compare to the prior and the likelihood?
Comment on the influence that the prior distribution has.
Does the Bayesian inference for this data appear to be highly sensitive to the choice of prior?
How does this compare to the $n=12$ situation?
\
\
\
\
\

1. If you had to produce a single number Bayesian estimate of $\theta$ based on the sample data, what number might you pick?
\
\
\
\
\


- In a Bayesian analysis, the posterior distribution contains all relevant information about parameters after observing sample data.
- We often use certain summary characteristics of the posterior distribution to make inferences about parameters.
- A **point estimate** of an unknown parameter is a single-number estimate of the parameter.
- Given a posterior distribution of a parameter $\theta$, three commonly used Bayesian point estimates of $\theta$ are:
    - the posterior mean
    - the posterior median
    - the posterior mode.
- In many situations, the posterior distribution will be roughly symmetric with a single peak, in which case posterior mean, median, and mode will all be about the same.
- Reducing the posterior distribution to a single-number point estimate loses a lot of the information the posterior distribution provides.
The entire posterior distribution quantifies the uncertainty about $\theta$ after observing sample data.
We will soon see how to more fully use the posterior distribution in making inference about $\theta$.




::: {#exm-kissing-data}
Continuing @exm-kissing-discrete where $\theta$ can only take values 0.1, 0.3, 0.5, 0.7, 0.9.
Consider a prior distribution which places probability 5/15, 4/15, 3/15, 2/15, 1/15 on the values 0.1, 0.3, 0.5, 0.7, 0.9, respectively.
Ssuppose that $y=8$ couples in a sample of size $n=12$ lean right.
Recall the Bayes table.
:::

1. Suppose we want a single number point estimate of $\theta$ *before* observing sample data.
Find the mode of the prior distribution of $\theta$, a.k.a., the "prior mode".
\
\
\
\
\

1. Find the expected value of the prior distribution of $\theta$, a.k.a., the "prior mean".
\
\
\
\
\

1. Suppose we want a single number point estimate of $\theta$ *after* observing sample data.
Find the mode of the posterior distribution of $\theta$, a.k.a., the "posterior mode".
\
\
\
\
\

1. Find the expected value of the prior distribution of $\theta$, a.k.a., the "posterior mean".
How does the posterior mean compare to the prior mean and the observed sample propotion?
\
\
\
\
\









## Notes


### Likelihood


If $\theta=0.1$ the likelihood of $y=8$ when $n=12$ is

```{r}
dbinom(8, size = 12, prob = 0.1)
```


If $\theta=0.3$ the likelihood of $y=8$ when $n=12$ is

```{r}
dbinom(8, 12, 0.3)
```

If $\theta=0.5$ the likelihood of $y=8$ when $n=12$ is

```{r}
dbinom(8, 12, 0.5)
```

If $\theta=0.7$ the likelihood of $y=8$ when $n=12$ is

```{r}
dbinom(8, 12, 0.7)
```

If $\theta=0.9$ the likelihood of $y=8$ when $n=12$ is

```{r}
dbinom(8, 12, 0.9)
```


### Prior = equally likely, discrete; data = 8/12





```{r}
# possible values of theta
theta = seq(0.1, 0.9, by = 0.2)

# prior distribution
prior = rep(1, length(theta))
prior = prior / sum(prior)

# observed data
n = 12
y_obs = 8

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()
```

- It is helpful to plot prior, likelihood, and posterior on the same plot.
- Since prior and likelihood are probability distributions, they are on the same scale.
- However, remember that the likelihood does not add up to anything in particular.
    - To put the likelihood on the same scale as prior and posterior, it is helpful to rescale the likelihood so that it adds up to 1.
    - Such a rescaling does not change the shape of the likelihood, it merely allows for easier comparison with prior and posterior.

```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |>
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "prob") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = prob,
             col = source)) +
  geom_point(size = 3) +
  geom_line(aes(linetype = source)) +
  scale_x_continuous(breaks = theta) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  labs(y = "Probability") +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```




### Prior = triangular, discrete; data = 8/12

The only code change is the `prior`

```{r}
# possible values of theta
theta = seq(0.1, 0.9, by = 0.2)

# prior distribution
prior = c(1, 2, 3, 2, 1)
prior = prior / sum(prior)

# observed data
n = 12
y_obs = 8

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()
```



```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |>
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "prob") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = prob,
             col = source)) +
  geom_point(size = 3) +
  geom_line(aes(linetype = source)) +
  scale_x_continuous(breaks = theta) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  labs(y = "Probability") +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = "left", discrete; data = 8/12

The only code change is the `prior`

```{r}
# possible values of theta
theta = seq(0.1, 0.9, by = 0.2)

# prior distribution
prior = c(5, 4, 3, 2, 1)
prior = prior / sum(prior)

# observed data
n = 12
y_obs = 8

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()
```



```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |>
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "prob") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = prob,
             col = source)) +
  geom_point(size = 3) +
  geom_line(aes(linetype = source)) +
  scale_x_continuous(breaks = theta) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  labs(y = "Probability") +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = equally likely, grid approximation; data = 8/12

The only real code change is to have a longer grid of `theta` values.
(There are some small changes to the plot, and only selected rows in the Bayes table are displayed.)

```{r}
# possible values of theta
theta = seq(0, 1, by = 0.0001)

# prior distribution
prior = rep(1, length(theta))
prior = prior / sum(prior)

# observed data
n = 12
y_obs = 8

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  # select a few rows to display
  slice(seq(5001, 7001, 250)) |> 
  kbl(digits = 8) |>
  kable_styling()
```


```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |> 
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "probability") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = probability,
             col = source)) +
  geom_line(aes(linetype = source), linewidth = 1) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = triangular, grid approximation; data = 8/12

The only change is the `prior`.

```{r}
# possible values of theta
theta = seq(0, 1, by = 0.0001)

# prior distribution
prior = 1 - 2 * abs(theta - 0.5)
prior = prior / sum(prior)

# observed data
n = 12
y_obs = 8

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  # select a few rows to display
  slice(seq(5001, 7001, 250)) |> 
  kbl(digits = 8) |>
  kable_styling()
```



```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |> 
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "probability") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = probability,
             col = source)) +
  geom_line(aes(linetype = source), linewidth = 1) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = "left", grid approximation; data = 8/12

The only change is the `prior`.

```{r}
# possible values of theta
theta = seq(0, 1, by = 0.0001)

# prior distribution
prior = 1 - theta
prior = prior / sum(prior)

# observed data
n = 12
y_obs = 8

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  # select a few rows to display
  slice(seq(5001, 7001, 250)) |> 
  kbl(digits = 8) |>
  kable_styling()
```



```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |> 
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "probability") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = probability,
             col = source)) +
  geom_line(aes(linetype = source), linewidth = 1) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = equally likely, grid approximation; data = 80/124

The only code change is to the observed data.

```{r}
# possible values of theta
theta = seq(0, 1, by = 0.0001)

# prior distribution
prior = rep(1, length(theta))
prior = prior / sum(prior)

# observed data
n = 124
y_obs = 80

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  # select a few rows to display
  slice(seq(5001, 7001, 250)) |> 
  kbl(digits = 8) |>
  kable_styling()
```


```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |> 
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "probability") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = probability,
             col = source)) +
  geom_line(aes(linetype = source), linewidth = 1) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = triangular, grid approximation; data = 80/124

The only change is the `prior`.

```{r}
# possible values of theta
theta = seq(0, 1, by = 0.0001)

# prior distribution
prior = 1 - 2 * abs(theta - 0.5)
prior = prior / sum(prior)

# observed data
n = 124
y_obs = 80

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  # select a few rows to display
  slice(seq(5001, 7001, 250)) |> 
  kbl(digits = 8) |>
  kable_styling()
```



```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |> 
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "probability") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = probability,
             col = source)) +
  geom_line(aes(linetype = source), linewidth = 1) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Prior = "left", grid approximation; data = 80/124

The only change is the `prior`.

```{r}
# possible values of theta
theta = seq(0, 1, by = 0.0001)

# prior distribution
prior = 1 - theta
prior = prior / sum(prior)

# observed data
n = 124
y_obs = 80

# likelihood of observed data for each theta
likelihood = dbinom(y_obs, n, theta)

# posterior is proportional to product of prior and likelihood
product = prior * likelihood
posterior = product / sum(product)

bayes_table = data.frame(theta,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  # select a few rows to display
  slice(seq(5001, 7001, 250)) |> 
  kbl(digits = 8) |>
  kable_styling()
```



```{r}
bayes_table |>
  # scale likelihood for plotting only
  mutate(likelihood = likelihood / sum(likelihood)) |> 
  select(theta, prior, likelihood, posterior) |>
  pivot_longer(!theta,
               names_to = "source",
               values_to = "probability") |>
  mutate(source = fct_relevel(source, "prior", "likelihood", "posterior")) |>
  ggplot(aes(x = theta,
             y = probability,
             col = source)) +
  geom_line(aes(linetype = source), linewidth = 1) +
  scale_color_manual(values = bayes_col) +
  scale_linetype_manual(values = bayes_lty) +
  theme_bw()
```

```{r}
# prior mean
sum(theta * prior)

# posterior mean
sum(theta * posterior)
```


### Simulating directly from the posterior distribution


```{r}
# simulate values of theta from the posterior distribution
theta_sim = sample(theta, 10000, replace = TRUE, prob = posterior)

# approximate posterior distribution
hist(theta_sim, freq = FALSE)

# approximate posterior mean
mean(theta_sim)

# approximate posterior median
median(theta_sim)

```


