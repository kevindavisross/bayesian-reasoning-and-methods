# Comparing Two Samples {#two-samples}


Most interesting statistical problems involve multiple unknown parameters.
For example, many problems involve comparing two (or more) populations or groups based on two (or more) samples.
In such situations, each population or group will have its own parameters, and there will often be dependence between parameters.
We are usually interested in difference or ratios of parameters between groups.

The example below concerns the familiar context of comparing two means.
However, the way independence is treated in the example is not the most common.
Soon we will see *hierarchical models* of dependence between parameters.



```{example, baby-smoke}

Do newborns born to mothers who smoke tend to weigh less at birth than newborns from mothers who don't smoke?
We'll investigate this question using birthweight (pounds) data on a sample of births in North Carolina over a one year period.

Assume birthweights follow a Normal distribution with mean $\mu_1$ for nonsmokers and mean $\mu_2$ for smokers, and standard deviation $\sigma$.
(We're assuming a common standard deviation to simplify a little, but we could also let standard deviation vary by smoking status.)

```

1. The prior distribution will be a joint distribution on $(\mu_1, \mu_2, \sigma)$ triples.
We could assume a prior under which $\mu_1$ and $\mu_2$ are independent.
But why might we want to assume some prior *dependence* between $\mu_1$ and $\mu_2$?
(For some motivation it might help to consider what the frequentist null hypothesis would be.)
1. One way to incorporate prior dependence is to assume a Multivariate Normal prior distribution.
For the prior assume:

    - $\sigma$ is independent of $(\mu_1, \mu_2)$
    - The precision $\tau=1/\sigma^2$ has a Gamma(1, 1) distribution
    - $(\mu_1, \mu_2)$ follow a Bivariate Normal distribution with prior means (7.5, 7.5) pounds, prior standard deviations (0.5, 0.5) pounds, and prior correlation 0.9.
    
    Simulate values of $(\mu_1, \mu_2)$ from the prior distribution^[Values from a Multivariate Normal distribution can be simulated using [`mvrnorm`](https://rdrr.io/cran/MASS/man/mvrnorm.html) from the `MASS` package. For Bivariate Normal, the inputs are the mean vector $[E(\mu_1), E(\mu_2)]$ and the covariance matrix
    \[
\begin{bmatrix}
	\textrm{Var}(\mu_1) & \textrm{Cov}(\mu_1, \mu_2) \\
	\textrm{Cov}(\mu_1, \mu_2) & \textrm{Var}(\mu_2) 
\end{bmatrix}
\]
where $\textrm{Cov}(\mu_1, \mu_2) = \textrm{Corr}(\mu_1, \mu_2)\textrm{SD}(\mu_1)\textrm{SD}(\mu_2)$. ]
and plot them.  Briefly^[Why briefly?  Because we want to focus on the *posterior* distribution.] describe the prior distribution.

1. How do you interpret the parameter $\mu_1 - \mu_2$?
Plot the prior distribution of $\mu_1-\mu_2$, and find a prior central 95% credible interval, and the prior probability that $\mu_1-\mu_2>0$.
1. Summarize, plot, and describe the sample data.
1. Is it reasonable to assume that the two *samples* are independent?  (In which case $\bar{y}_1$ and $\bar{y}_2$ are independent conditional on $(\mu_1, \mu_2, \sigma)$.)
How would you compute the likelihood?
1. Use JAGS to approximate the posterior distribution.
(The coding is a little tricky.
See the code and some comments below.)
Plot the posterior distribution.
How strong is the dependence between $\mu_1$ and $\mu_2$ in the posterior?
Why do you think that is?
1. Plot the posterior distribution of $\mu_1-\mu_2$, describe it, find and interpret a posterior central 95% credible interval, and find the posterior probability that $\mu_1-\mu_2>0$.
1. If we're interested in $\mu_1-\mu_2$, why didn't we put a prior directly on $\mu_1-\mu_2$ rather than on $(\mu_1,\mu_2)$?
1. Plot the posterior distribution of $\mu_1/\mu_2$, describe it, and find and interpret a posterior central 95% credible interval.
1. Is there some evidence that babies whose mothers smoke tend to weigh less than those whose mothers don't smoke? Can we say that smoking is the cause of the difference in mean weights?
1. Is there some evidence that babies whose mothers smoke tend to weigh *much* less than those whose mothers don't smoke?
1. One quantity of interest is the **effect size**, which is a way of measuring the magnitude of the difference between groups.  When comparing two means, a simple measure of effect size (*Cohen's $d$*) is
\[
\frac{\mu_1 - \mu_2}{\sigma}
\]
Plot the posterior distribution of this effect size, describe it, and find and interpret a posterior central 95% credible interval.




```{solution}
to Example \@ref(exm:baby-smoke)
```


1. If our prior belief is that there is no difference in mean birthweights between babies of smokers and non-smokers, then our prior should place high probability on $\mu_1$ being close to $\mu_2$.
Even if we want our prior to allow for different distributions for $\mu_1$ and $\mu_2$, there might still be some dependence.
For example, we would assign a different prior conditional probability to the event that $\mu_1 > 8.5$ given $\mu_2 > 8.5$ than we would given $\mu_2<7.5$.
Our prior uncertainty about mean birthweights of babies of nonsmokers informs our prior uncertainty about mean birthweights of babies in general, and hence also of babies of smokers.
1. Our main focus is on $(\mu_1, \mu_2)$.
(The SD $\sigma$ is of secondary interest, and its prior wasn't the greatest choice anyway.)
We see that the prior places high density on $(\mu_1, \mu_2)$ pairs with $\mu_1$ close to $\mu_2$.


    ```{r, fig.show="hold", out.width="50%"}
mu_prior_mean <- c(7.5, 7.5)
mu_prior_sd <- c(0.5, 0.5)
mu_prior_corr <- 0.9

mu_prior_cov <- matrix(c(mu_prior_sd[1] ^ 2,
                  mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2],
                  mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2],
                  mu_prior_sd[2] ^ 2), nrow = 2)

library(MASS)

theta_sim_prior = data.frame(mvrnorm(10000, mu_prior_mean, mu_prior_cov),
                               rgamma(10000, 1, 1))

names(theta_sim_prior) = c("mu1", "mu2", "tau")

hist(theta_sim_prior$mu1, xlab = "mu1")

ggplot(theta_sim_prior, aes(mu1, mu2)) +
  geom_point(color = "skyblue", alpha = 0.4) +
  geom_density_2d(color = "orange", size = 1) +
  geom_abline(intercept = 0, slope = 1, color="black", 
                 linetype="dashed")


```


1. The parameter $\mu_1 - \mu_2$ is the difference in mean birthweights, smokers minus non-smokers.
The prior mean of $\mu_1-\mu_2$ is 0, reflecting a prior belief towards no difference in mean birthweight between smokers and non-smokers.
Furthermore, there is a fairly high prior probability that the mean birthweight for smokers is close to the mean birthweight for non-smokers, with a difference of at most about 0.5 pounds.
Under this prior, nonsmokers and smokers are equally likely to have the higher mean birthweight.


    ```{r}
mu_diff = theta_sim_prior$mu1 - theta_sim_prior$mu2

hist(mu_diff, xlab = "mu1 - mu2")
abline(v = quantile(mu_diff, c(0.025, 0.975)), lty = 2)

quantile(mu_diff, c(0.025, 0.975))

sum(mu_diff > 0 ) / 10000
```

1. The distributions of birthweights are fairly similar for smokers and non-smokers.
The sample mean birthweight for smokers is about 0.3 pounds less than the sample mean birthweight for smokers.
The sample SDs of birthweights are similar for both groups, around 1.4-1.5 pounds.


    ```{r}
data = read.csv("_data/baby_smoke.csv")

ggplot(data, aes(weight, fill = habit)) + 
   geom_histogram(alpha = 0.3, aes(y = ..density..), position = 'identity')

data %>%
  group_by(habit) %>%
  summarize(n(), mean(weight), sd(weight)) %>%
  kable(digits = 2)

```

1. Yes, it is reasonable to assume that the two *samples* are independent.
The data for smokers was collected separately from the data for non-smokers.
    For each observed value of birthweight for non-smokers, evaluate the likelihood based on a $N(\mu_1, \sigma)$ distribution.
For example, if birthweight of a non-smoker is 8 pounds, the likelihood is `dnorm(8, mu1, sigma)`; if birthweight of a non-smoker is 7 pounds, the likelihood is `dnorm(7, mu1, sigma)`.
The likelihood for the sample of non-smokers would be the products --- assuming independence *within* sample --- of the likelihoods of the individual values, as a function of $\mu_1$ and $\sigma$: `dnorm(8, mu1, sigma) * dnorm(7, mu1, sigma) * ...`

    The likelihood for the sample of smokers would be the products of the likelihoods of the individual values, as a function of $\mu_2$ and $\sigma$: `dnorm(8.3, mu2, sigma) * dnorm(7.1, mu2, sigma) * ...`
    
    The likelihood function for the full sample would be the product --- assuming independence *between* samples --- of the likelihoods for the two samples, a function of $\mu_1, \mu_2$ and $\sigma$. 
    
1. Here is the code; there are some comments about syntax at the end of this chapter.

    ```{r}
# data
data = read.csv("_data/baby_smoke.csv")

y = data$weight

x = (data$habit == "smoker") + 1

n = length(y)

n_groups = 2

# Prior parameters
mu_prior_mean <- c(7.5, 7.5)
mu_prior_sd <- c(0.5, 0.5)
mu_prior_corr <- 0.9

mu_prior_cov <- matrix(c(mu_prior_sd[1] ^ 2,
                  mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2],
                  mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2],
                  mu_prior_sd[2] ^ 2), nrow = 2)

# Model
model_string <- "model{

  # Likelihood
  for (i in 1:n){
      y[i] ~ dnorm(mu[x[i]], tau)
  }

  # Prior
  mu[1:n_groups] ~ dmnorm.vcov(mu_prior_mean[1:n_groups], mu_prior_cov[1:n_groups, 1:n_groups])

  tau ~ dgamma(1, 1)

}"

dataList = list(y = y, x = x, n = n, n_groups = n_groups,
                mu_prior_mean = mu_prior_mean, mu_prior_cov = mu_prior_cov)

# Compile
model <- jags.model(textConnection(model_string),
                    data = dataList,
                    n.chains = 5)

# Simulate
update(model, 1000, progress.bar = "none")

Nrep = 10000

posterior_sample <- coda.samples(model,
                                 variable.names = c("mu", "tau"),
                                 n.iter = Nrep,
                                 progress.bar = "none")

# Summarize and check diagnostics
summary(posterior_sample)
plot(posterior_sample)

theta_sim_posterior = as.matrix(posterior_sample)
head(theta_sim_posterior)
theta_sim_posterior = as.data.frame(theta_sim_posterior)
names(theta_sim_posterior) = c("mu1", "mu2", "tau")

ggplot(theta_sim_posterior, aes(mu1, mu2)) +
  geom_point(color = "skyblue", alpha = 0.4) +
  geom_density_2d(color = "orange", size = 1) +
  geom_abline(intercept = 0, slope = 1, color="black", 
                 linetype="dashed")

cor(theta_sim_posterior$mu1, theta_sim_posterior$mu2)
```


    The posterior mean of $\mu_1$ is close to the sample mean birthweight for non-smokers, The posterior mean of $\mu_2$ is close to the sample mean birthweight for smokers.
The posterior SD of $\mu_1$ is smaller than that of $\mu_2$, reflecting the larger sample size for non-smokers than smokers.
The posterior distribution places most of its probability on $(\mu_1, \mu_2)$ pairs with $\mu_1>\mu_2$, representing a much stronger belief (than prior) that mean birthweight for smokers is less than for non-smokers.
The posterior correlation between $\mu_1$ and $\mu_2$ is about 0.1, which is much smaller than the prior correlation.
Even though there was fairly strong dependence between $\mu_1$ and $\mu_2$ in the prior, there was *independence between the samples in the data*, represented in the likelihood.
With the large sample sizes (especially for non-smokers) the data has more influence on the posterior than the prior does.

1. The posterior distribution of $\mu_1-\mu_2$ is approximately Normal.
The posterior mean of $\mu_1-\mu_2$ is about 0.22 pounds, which is a compromise between the prior mean of $\mu_1-\mu_2$ of 0 (no difference) and the difference in sample means of about 0.32 pounds.
There is a posterior probability of about 96% that the mean birthweight for non-smokers is less than the mean birthweight for smokers.
There is posterior probability of 95% that the mean birthweight for smokers is between 0.45 pounds less than and 0.02 pounds greater than the mean birthweight for smokers.


    ```{r}
mu_diff = theta_sim_posterior$mu1 - theta_sim_posterior$mu2

hist(mu_diff, xlab = "mu1 - mu2")
abline(v = quantile(mu_diff, c(0.025, 0.975)), lty = 2)

quantile(mu_diff, c(0.025, 0.975))

mean(mu_diff)

sum(mu_diff > 0 ) / 50000
```

1. Putting a prior directly on $\mu_1-\mu_2$ does allow us to make inference about the difference in mean birthweights.
But what if we also want to estimate the mean birthweight for each group?
Having a posterior distribution just on the difference between the two groups does not allow us to estimate the mean for either group.
Also, $\mu_1-\mu_2$ is the absolute difference in means, but what if we want to measure the difference in relative terms?
Putting a prior distribution on $(\mu_1, \mu_2)$ enables us to make posterior inference about mean birthweight for both non-smokers and smokers and any parameter (difference, ratio) that depends on the means.

1. The posterior distribution of $\mu_1/\mu_2$ is approximately Normal.
The posterior mean of $\mu_1/\mu_2$ is about 1.03, which is a compromise between the prior mean of $\mu_1/\mu_2 = 1$ (no difference) and the ratio of sample means of 1.046.
There is a posterior probability of 95% that the mean birthweight of non-smokers is between 0.998 and 1.067 *times* greater than the mean birthweight of smokers.

    ```{r}
mu_ratio = theta_sim_posterior$mu1 / theta_sim_posterior$mu2

hist(mu_ratio, xlab = "mu1 / mu2")
abline(v = quantile(mu_ratio, c(0.025, 0.975)), lty = 2)

quantile(mu_ratio, c(0.025, 0.975))

mean(mu_ratio)

```
1. Yes, there is some evidence.
Even though we started with fairly strong prior credibility of no difference, with the relatively large sample sizes, the difference in sample means observed in the data was enough to overturn the prior beliefs.
Now, the 95% credible interval for $\mu_1-\mu_2$ does contain 0, indicating some plausibility of no difference. 
But there's nothing special about 95% credibility, and we should look at the whole posterior distribution of $\mu_1-\mu_2$.
According to our posterior distribution, we place a high degree of plausibility on the mean birthweight for smokers being less than the mean birthweight of non-smokers.

    The question of causation has nothing to do with whether we are doing a Bayesian or frequentist analysis.  Rather, the question of causation concerns: *how were the data collected?*  In particular, was this an experiment with random assignment of the explanatory variable?  It wasn't; it was an observational study (you can't randomly assign some mothers to smoke). Therefore, there is potential for confounding variables. Maybe mothers who smoke tend to be less healthy in general than mothers who don't smoke, and maybe some other aspect of health is more closely associated with lower birthweight than smoking is.
    
1. The posterior distribution of $\mu_1-\mu_2$ does not give much plausibility to large differences in mean birthweight.
Almost all of the posterior probability is placed on the absolute difference being less than 0.5 pounds, and the relative difference being no more than 1.07 times.
Just because we have evidence that there is a difference, doesn't necessarily mean that the difference is large in practical terms.
 
1. The observed effect size is about 0.3/1.5 = 0.2.
Birthweights vary naturally from baby to baby by about 1.5 pounds, so a difference of 0.3 pounds seems relatively small.
The sample mean birthweight for non-smokers is *0.2 standard deviations* greater than the sample mean birthweight for smokers.
The posterior mean of $(\mu_1-\mu_2)/\sigma$ is about 0.15, which is a compromise between the prior mean of $(\mu_1-\mu_2)/\sigma$ of 0 (no difference) and the sample effect size of 0.2.
There is a posterior probability of 95% that the mean birthweight for non-smokers is at most *0.27 standard deviations* above the mean birthweight for smokers.
This is a pretty small effect size.
(Of course, smoking has many other adverse health effects.
But looking at birthweight alone, based on this data set we cannot conclude that there is a large difference in mean birthweight between smokers and non-smokers.)


    ```{r}
sigma = 1 / sqrt(theta_sim_posterior$tau)
    
effect_size = mu_diff / sigma

hist(effect_size, xlab = "effect size")
abline(v = quantile(effect_size, c(0.95)), lty = 2)

quantile(effect_size, c(0.95))

mean(effect_size)

```


It is typical to assume *independence in the data*, e.g., independence of values of the measured variables within and between samples (conditional on the parameters).
Whether independence in the data is a reasonable assumption depends on how the data is collected.

But whether it is reasonable to assume prior *independence of parameters* is a completely separate question and is dependent upon our subjective beliefs about any relationships between parameters.

The primary output of a Bayesian data analysis is the full joint posterior distribution on all parameters.
Given the joint distribution, the distribution of transformations of the primary parameters is readily obtained.

When comparing groups, a more important question than "is there a difference?" is "*how large* is the difference?"
An **effect size** is a measure of the magnitude of a difference between groups.
A difference in parameters can be used to measure the absolute size of the difference in the measurement units of the variable, but effect size can also be measured as a relative difference.

When comparing a numerical variable between two groups, one measure of the population effect size is **Cohens's $d$**
\[
\frac{\mu_1 - \mu_2}{\sigma}
\]

The values of any numerical variable vary naturally from unit to unit.
The SD of the numerical variable measures the degree to which individual values of the variable vary naturally, so the SD provides a natural "scale" for the variable.
Cohen's $d$ compares the magnitude of the difference in means relative to the natural scale (SD) for the variable


Some rough guidelines for interpreting $|d|$:

|             |       |        |       |            |      |
|------------:|------:|-------:|------:|-----------:|-----:|
|         *d* |   0.2 |    0.5 |   0.8 |        1.2 |  2.0 |
| Effect size | Small | Medium | Large | Very Large | Huge |
|             |       |        |       |            |      |

For example, assume the two population distributions are Normal and the two population standard deviations are equal. Then when the effect size is 1.0 the median of the distribution with the higher mean is the 84th percentile of the distribution with the lower mean, which is a very large difference.

|                                    |       |        |       |      |            |      |
|:-----------------------------------|:-----:|:------:|:-----:|:----:|:----------:|:----:|
| *d*                                |  0.2  |  0.5   |  0.8  | 1.0  |    1.2     | 2.0  |
| Effect size                        | Small | Medium | Large |      | Very Large | Huge |
| Median of population 1 is          |       |        |       |      |            |      |
| (blank) percentile of population 2 | 58th  |  69th  | 79th  | 84th |    89th    | 98th |


```{r, echo = FALSE}
knitr::include_graphics("_graphics/effect_size_plots.png")
```


**A few notes on the JAGS code.**

- You should be able to define the prior parameters for the Multivariate Normal distribution within JAGS, but I keep getting an error.
So I'm defining prior parameters outside of JAGS and then passing them in with the data.
(I can never remember what you can do in JAGS and what you need to do in R and pass to JAGS.)
- The Bivariate Normal prior is coded in JAGS using `dmnorm.vcov` which has two parameters: a mean *vector* and a covariance *matrix*.  (There is also `dmnorm` is which is parametrized by the precision matrix.)
- The prior `mu[1:2] ~ dmnorm(...)` creates a vector `mu` with two components `mu[1]` and `mu[2]`.  When `"mu"` is called in the  `variable.names = c("mu", "tau")` argument of `coda.samples` JAGS will return the vector `mu` --- that is, both components `mu[1]` and `mu[2]`.  See the output of posterior_sample.
- Group variables (like non-smoker/smoker) need to be coded as numbers in JAGS, starting with 1.  So `x` recodes smoking status as 1 for non-smokers and 2 for smokers.
- We have data on individual birthweights, so we evaluate the likelihood of each individual value `y[i]` using a Normal distribution and then use a for loop to find the likelihood for the sample.
- Notice that the mean used in the likelihood depends on the group: `mu[x[i]]`.
    For example, if element `i` has birthweight `y[i] = 8` and is a non-smoker `x[i] = 1`, then the likelihood is evaluated using a Normal distribution with mean $\mu_1$; for this element `y[i] ~ dnorm(mu[x[i]], ...)` in JAGS is like calling `dnorm(y[i], mu[x[i]], ...) = dnorm(8, mu[1], ...)` in R.
    If element `i` has birthweight `y[i] = 7.3` and is a smoker `x[i] = 2`, then the likelihood is evaluated using a Normal distribution with mean $\mu_2$; for this element `y[i] ~ dnorm(mu[x[i]], ...)` in JAGS is like calling `dnorm(y[i], mu[x[i]], ...) = dnorm(7.3, mu[2], ...)` in R.
- The  `variable.names = c("mu", "tau")` argument of `coda.samples` tells JAGS which simulation output to save.  Given the joint posterior distributon of the primary parameters, it is relatively easy to obtain the posterior distribution of transformations of these parameters outside of JAGS in R.


<!-- ```{example, penguins} -->

<!-- Many studies of animals involve tagging the animals, but can the tags be harmful? -->
<!-- Spefically, are metal bands used for tagging penguins? -->
<!-- [Researchers studied the effects of banding](https://www.nature.com/articles/nature09630) in a sample of 100 penguins near Antarctica that had already been tagged with RFID (radio frequency identification) chips. -->
<!-- Of these 100 penguins, 50 were randomly assigned to receive metal bands on their flippers (in addition to the RFID chip), while the remaining 50 did not receive metal bands. -->
<!-- The researchers kept track of which penguins survived for the 4.5-year study and which did not. -->
<!-- Of the 50 penguins without metal bands, 31 survived this 4.5-year period. -->
<!-- Of the 50 penguins with metal bands, 16 survived the 4.5-year period. -->

<!-- Let $\theta_1$ denote the probability that a penguin *without* a metal band survives a 4.5-year period, and let $\theta_2$ denote the probability that a penguin *with* a metal band survives a 4.5-year period. -->

<!-- We'll use the study data to find the posterior distribution of $(\theta_1, \theta_2)$ and related parameters. -->

<!-- ``` -->

<!-- 1. The prior distribution will be a joint distribution on $(\theta_1 \theta_2)$ pairs. -->
<!-- We could assume a prior under which $\theta_1$ and $\theta_2$ are independent. -->
<!-- But why might we want to assume some prior *dependence* between $\theta_1$ and $\theta_2$? -->
<!-- (For some motivation it might help to consider what the frequentist null hypothesis would be.) -->
<!-- 1. One way to incorporate prior dependence is to assume a Bivariate Normal prior distribution.Plot the prior distribution of $(\theta_1, \theta_2)$. -->
<!-- 1. How do you interpret the parameter $\theta_1 - \theta_2$? Sketch its prior distribution and find a prior 99% credible interval. -->
<!-- 1. Is it reasonable to assume that the two *samples* are independent?  (In which case $\hat{p}_1$ and $\hat{p}_2$ are independent conditional on $(\theta_1, \theta_2)$.)  What is the likelihood? -->
<!-- 1. Without writing out the full prior and likelihood, identify the posterior distribution of $(\theta_1, \theta_2)$. -->
<!-- 1. Sketch the posterior distribution of $\theta_1 - \theta_2$ and find and interpret a posterior 99% credible interval.  Find the posterior mean and compare. -->
<!-- 1. Why didn't we just put a prior on $\theta_1-\theta_2$? -->
<!-- 1. How do you interpret the parameter $\theta_1 / \theta_2$? Use simulation to approximate the posterior distribution of $\theta_1/\theta_2$, and find and interpret a posterior 99% credible interval. -->
<!-- 1. Is there evidence that metal bands are harmful?  Can we conclude cause? -->

<!-- ```{r} -->
<!-- mu_prior_mean <- c(7.5, 7.5) -->
<!-- mu_prior_sd <- c(0.5, 0.5) -->
<!-- mu_prior_corr <- 0.9 -->

<!-- mu_prior_cov <- matrix(c(mu_prior_sd[1] ^ 2, -->
<!--                   mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2], -->
<!--                   mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2], -->
<!--                   mu_prior_sd[2] ^ 2), nrow = 2) -->

<!-- library(MASS) -->

<!-- theta_sim_prior = data.frame(mvrnorm(10000, mu, Sigma)) -->
<!-- names(theta_sim_prior) = c("theta1", "theta2") -->
<!-- theta_sim_prior[, "distribution"] = "prior" -->

<!-- hist(theta_sim_prior$theta1 - theta_sim_prior$theta2) -->

<!-- theta_sim_post = data.frame(as.matrix(posterior_sample)) -->
<!-- names(theta_sim_post) = c("theta1", "theta2") -->
<!-- theta_sim_post[, "distribution"] = "posterior" -->

<!-- theta_sim = bind_rows(theta_sim_prior, theta_sim_post) -->

<!-- ggplot(theta_sim, aes(theta1, theta2, col = distribution)) + -->
<!--   scale_color_manual(values = c("orange", "grey")) + -->
<!--   geom_point(alpha = 0.1) + -->
<!--   stat_ellipse() -->
<!-- #  geom_density_2d(size = 1) -->

<!-- ``` -->


<!-- ```{r} -->

<!-- # Data -->
<!-- data = read.csv("_data/baby_smoke.csv") -->

<!-- y = data$weight -->

<!-- x = (data$habit == "smoker") + 1 -->

<!-- n = length(y) -->

<!-- n_groups = 2 -->

<!-- # Prior parameters -->
<!-- mu_prior_mean <- c(7.5, 7.5) -->
<!-- mu_prior_sd <- c(0.5, 0.5) -->
<!-- mu_prior_corr <- 0.9 -->

<!-- mu_prior_cov <- matrix(c(mu_prior_sd[1] ^ 2, -->
<!--                   mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2], -->
<!--                   mu_prior_corr * mu_prior_sd[1] * mu_prior_sd[2], -->
<!--                   mu_prior_sd[2] ^ 2), nrow = 2) -->

<!-- # Model -->
<!-- model_string <- "model{ -->

<!--   # Likelihood -->
<!--   for (i in 1:n){ -->
<!--     y[i] ~ dnorm(mu[x[i]], tau) -->
<!--   } -->

<!--   # Prior -->
<!--   mu[1:n_groups] ~ dmnorm.vcov(mu_prior_mean[1:n_groups], mu_prior_cov[1:n_groups, 1:n_groups]) -->

<!--   tau ~ dgamma(1, 1) -->

<!-- }" -->

<!-- dataList = list(y = y, x = x, n = n, n_groups = n_groups, -->
<!--                 mu_prior_mean = mu_prior_mean, mu_prior_cov = mu_prior_cov) -->

<!-- # Compile -->
<!-- model <- jags.model(textConnection(model_string),  -->
<!--                     data = dataList, -->
<!--                     n.chains = 5) -->

<!-- # Simulate -->
<!-- update(model, 1000, progress.bar = "none") -->

<!-- Nrep = 10000 -->

<!-- posterior_sample <- coda.samples(model,  -->
<!--                                  variable.names = c("mu", "tau"),  -->
<!--                                  n.iter = Nrep, -->
<!--                                  progress.bar = "none") -->

<!-- # Summarize and check diagnostics -->
<!-- summary(posterior_sample) -->
<!-- plot(posterior_sample) -->

<!-- thetas = as.matrix(posterior_sample) -->

<!-- ``` -->





<!-- Once the full joint posterior distribution on parameters is known (or approximated), the posterior distribution of transformations of parameters can be easily approximated via simulation -->



<!-- You should be able to define the prior parameters within JAGS, but I keep getting an error. -->
<!-- So I'm defining prior parameters outside of JAGS and then passing them in with the data. -->

<!-- A Bivariate Normal distributio has two parameters: a mean *vector* and a covariance *matrix*.  (The `dmnorm.vcov` is parameterized by the covariance matrix,.) -->


<!-- ```{r} -->
<!-- mu <- c(0.5, 0.5) -->
<!-- sigma <- c(0.25, 0.25) -->
<!-- rho <- 0.99 -->

<!-- Sigma <- matrix(c(sigma[1] ^ 2, rho * sigma[1] * sigma[2], rho * sigma[1] * sigma[2], sigma[2] ^ 2), nrow = 2) -->

<!-- library(MASS) -->

<!-- theta_sim_prior = data.frame(mvrnorm(10000, mu, Sigma)) -->
<!-- names(theta_sim_prior) = c("theta1", "theta2") -->
<!-- theta_sim_prior[, "distribution"] = "prior" -->

<!-- hist(theta_sim_prior$theta1 - theta_sim_prior$theta2) -->

<!-- theta_sim_post = data.frame(as.matrix(posterior_sample)) -->
<!-- names(theta_sim_post) = c("theta1", "theta2") -->
<!-- theta_sim_post[, "distribution"] = "posterior" -->

<!-- theta_sim = bind_rows(theta_sim_prior, theta_sim_post) -->

<!-- ggplot(theta_sim, aes(theta1, theta2, col = distribution)) + -->
<!--   scale_color_manual(values = c("orange", "grey")) + -->
<!--   geom_point(alpha = 0.1) + -->
<!--   stat_ellipse() -->
<!-- #  geom_density_2d(size = 1) -->

<!-- ``` -->


<!-- ```{r} -->

<!-- # Data -->
<!-- n = c(50, 50) -->
<!-- y = c(31, 16) -->

<!-- # Prior parameters -->
<!-- mu <- c(0.5, 0.5) -->
<!-- sigma <- c(3, 3) -->
<!-- rho <- 0.99 -->

<!-- Sigma <- matrix(c(sigma[1] ^ 2, rho * sigma[1] * sigma[2], rho * sigma[1] * sigma[2], sigma[2] ^ 2), nrow = 2) -->

<!-- # Model -->
<!-- model_string <- "model{ -->

<!--   # Likelihood -->
<!--   for (i in 1:2){ -->
<!--     y[i] ~ dbinom(theta[i], n[i]) -->
<!--   } -->

<!--   # Prior -->
<!--   theta[1:2] ~ dmnorm.vcov(mu[1:2], Sigma[1:2, 1:2]) -->

<!-- }" -->

<!-- dataList = list(y = y, n = n, mu = mu, Sigma = Sigma) -->

<!-- # Compile -->
<!-- model <- jags.model(file = textConnection(model_string), -->
<!--                     data = dataList) -->

<!-- model <- jags.model(textConnection(model_string), -->
<!--                     data = dataList, -->
<!--                     n.chains = 5) -->

<!-- # Simulate -->
<!-- update(model, 1000, progress.bar = "none") -->

<!-- Nrep = 10000 -->

<!-- posterior_sample <- coda.samples(model, -->
<!--                                  variable.names = c("theta"), -->
<!--                                  n.iter = Nrep, -->
<!--                                  progress.bar = "none") -->

<!-- # Summarize and check diagnostics -->
<!-- summary(posterior_sample) -->
<!-- plot(posterior_sample) -->

<!-- thetas = as.matrix(posterior_sample) -->
<!-- plot(thetas) -->
<!-- ``` -->


