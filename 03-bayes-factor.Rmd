# Odds and Bayes Factors {#bayes-factor}





```{example bayes-false-positive}

The ELISA test for HIV was widely used in the mid-1990s for screening blood donations.  As with most medical diagnostic tests, the ELISA test is not perfect.  If a person actually carries the HIV virus, experts estimate that this test gives a positive result 97.7% of the time.  (This number is called the *sensitivity* of the test.)  If a person does not carry the HIV virus, ELISA gives a negative (correct) result 92.6% of the time (the *specificity* of the test).  Estimates at the time were that 0.5% of the American public carried the HIV virus (the *base rate*).

Suppose that a randomly selected American tests positive; we are interested in the conditional probability that the person actually carries the virus.

 
```

1. Before proceeding, make a guess for the probability in question.
\[
0-20\% \qquad 20-40\% \qquad 40-60\% \qquad 60-80\% \qquad 80-100\%
\]
1. Denote the probabilities provided in the setup using proper notation
1. Construct an appropriate two-way table and use it to compute the probability of interest.
1. Construct a Bayes table and use it to compute the probability of interest.
1. Explain why this probability is small, compared to the sensitivity and specificity.
1. By what factor has the probability of carrying HIV increased, given a positive test result, as compared to before the test?



```{solution, bayes-false-positive-sol}
to Example \@ref(exm:bayes-false-positive)
```

```{asis, fold.chunk = TRUE}

1. We don't know what you guessed, but from experience many people guess 80-100%.  Afterall, the test is correct for most of people who carry HIV, and also correct for most people who don't carry HIV, so it seems like the test is correct most of the time.  But this argument ignores one important piece of information that has a huge impact on the results: most people do not carry HIV.
1. Let $H$ denote the event that the person carries HIV (hypothesis), and let $E$ denote the event that the test is positive (evidence). Therefore, $H^c$ is the event that the person does not carry HIV, another hypothesis. We are given
    - prior probability: $P(H) = 0.005$
    - likelihood of testing positive, if the person carries HIV: $P(E|H) = 0.977$
    - $P(E^c|H^c) = 0.926$
    - likelihood of testing positive, if the person does not carry HIV: $P(E|H^c) = 1-P(E^c|H^c) = 1-0.926 = 0.074$
    - We want to find the posterior probability $P(H|E)$.
1. Considering a hypothetical population of Americans (at the time)
    - 0.5% *of Americans* carry HIV
    - 97.7% *of Americans who carry HIV* test positive
    - 92.6% *of Americans who do not carry HIV* test negative
    - We want to find the percentage *of Americans who test positive* that carry HIV.  
1. Assuming 1000000 Americans

    |                    | Tests positive | Does not test positive |   Total |
    |--------------------|---------------:|-----------------------:|--------:|
    | Carries HIV        |           4885 |                    115 |    5000 |
    | Does not carry HIV |          73630 |                 921370 |  995000 |
    | Total              |          78515 |                 921485 | 1000000 |

    Among the 78515 who test positive, 4885 carry HIV, so the probability that an American who tests positive actually carries HIV is 4885/78515 = 0.062.
1. See the Bayes table below.
1. The result says that only 6.2% *of Americans who test positive* actually carry HIV.  It is true that the test is correct for most Americans with HIV (4885 out of 5000) and incorrect only for a small proportion of Americans who do not carry HIV (73630 out of 995000).  But since so few Americans carry HIV, the sheer *number* of false positives (73630) swamps the *number* of true positives (4885).
1. Prior to observing the test result, the prior probability that an American carries HIV is $P(H) = 0.005$.  The posterior probability that an American carries HIV given a positive test result is $P(H|E)=0.062$.
\[
  \frac{P(H|E)}{P(H)} = \frac{0.062}{0.005} =  12.44
\]
    An American who tests positive is about 12.4 times more likely to carry HIV than an American whom the test result is not known.  So while 0.067 is still small in absolute terms, the posterior probability is much larger relative to the prior probability.

```


```{r, echo = FALSE}

hypothesis = c("Carries HIV", "Does not carry HIV")

prior = c(0.005, 1 - 0.005)

likelihood = c(0.977, 1 - 0.926) # given positive test

product = prior * likelihood

posterior = product / sum(product)

bayes_table = data.frame(hypothesis,
                         prior,
                         likelihood,
                         product,
                         posterior) %>%
  add_row(hypothesis = "sum",
          prior = sum(prior),
          likelihood = NA,
          product = sum(product),
          posterior = sum(posterior))

kable(bayes_table, digits = 4, align = 'r')

```


Remember, the conditional probability of $H$ given $E$, $P(E|H)$,  is not the same as the conditional probability of $E$ given $H$, $P(E|H)$, and they can be vastly different. It is helpful to think of probabilities as percentages and ask "percent of what?" For example, the percentage of *people who carry HIV* that test positive is a very different quantity than the percentage of *people who test positive* that carry HIV. Make sure to properly identify the "denominator" or baseline group the percentages apply to.

Posterior probabilities can be highly influenced by the original prior probabilities, sometimes called the **base rates**.  . The example illustrates that when the base rate for a condition is very low and the test for the condition is less than perfect there will be a relatively high probability that a positive test is a *false positive.*  Don't [neglect the base rates](https://en.wikipedia.org/wiki/Base_rate_fallacy) when evaluating posterior probabilities





```{example}

True story: On a camping trip in 2003, my wife and I were driving in Vermont when, suddenly, a very large, hairy, black animal lumbered across the road in front of us and into the woods on the other side.  It happened very quickly, and at first I said "It's a gorilla!"  But then after some thought, and much derision from my wife, I said "it was probably a bear."

I think this story provides an anecdote about Bayesian reasoning, albeit bad reasoning at first but then good.  Put the story in a Bayesian context by identifying hypotheses, evidence, prior, and likelihood.  What was the mistake I made initially?

```

```{asis, fold.chunk = TRUE}

- "Type of animal" is playing the role of the hypothesis: gorilla, bear, dog, squirrel, rabbit, etc.
- That the animal is very large, hairy, and black is the evidence.
- The likelihood value for the animal being very large, hairy, and black is close to 1 for both a bear and gorilla, maybe more middling for a dog, but close to 0 for a squirrel, rabbit, etc.

The mistake I made initially was to neglect the base rates and not consider my prior probabilities.  Let's say the likelihood is 1 for both gorilla and bear and 0 for all other animals.  Then based solely on the likelihoods, the posterior probability would be 50/50 for gorilla and bear, which maybe is why I guessed gorilla.

After my initial reaction, I paused to formulate my prior probabilities, which considering I was in Vermont, gave much higher probability to a bear than a gorilla. (My prior probabilities should also have given even higher probability to animals such as dogs, squirrels, and rabbits.)

By combining prior and likelihood in the appropriate way, the posterior probability is

- very high for a bear, due to high likelihood and not-too-small prior,
- close to 0 for a gorilla, due to the very small prior,
- and very low for a squirrel or rabbit  or other small animals because of the close-to-zero likelihood, even if the prior is large.

```


Recall that the odds of an event is a ratio involving the probability that the event occurs and the probability that the event does not occur
\[
\text{odds}(A)  = \frac{P(A)}{P(A^c)} = \frac{P(A)}{1-P(A)}
\]
In many situations (e.g.\ gambling) odds are reported as odds \emph{against} $A$, that is, the odds of $A^c$: $P(A^c)/P(A)$.

The probability of an even can be obtained from odds
\[
P(A) = \frac{\text{odds}(A)}{1+\text{odds}(A)}
\]




```{example, bayes-false-positive-odds}

Continuing Example \@ref(exm:bayes-false-positive)

```


1. In symbols and words, what does one minus the answer to the probability in question in Example \@ref(exm:bayes-false-positive) represent? 
1. Calculate the *prior odds* of a randomly selected American having the HIV virus, before taking an ELISA test.
1. Calculate the *posterior odds* of a randomly selected American having the HIV virus, given a positive test result.
1. By what factor has the *odds* of carrying HIV increased, given a positive test result, as compared to before the test?  This is called the **Bayes factor**.
1. Suppose you were given the prior odds and the Bayes factor.  How could you compute the posterior odds?
1. Compute the ratio of the likelihoods of testing positive, for those who carry HIV and for those who do not carry HIV.  What do you notice?




```{solution, bayes-false-positive-odds-sol}

to Example \@ref(exm:bayes-false-positive-odds)

```

```{asis, fold.chunk = TRUE}

1. $1-P(H|E) = P(H^c|E)=0.938$ is the posterior probability that an American who has a positive test does not carry HIV.
1. The prior probability of carrying HIV is $P(H)=0.005$ and the prior probability of not carrying HIV is $P(H^c) = 1-0.005 = 0.995$
\[
\frac{P(H)}{P(H^c)} = \frac{0.005}{0.995} = \frac{1}{199} \approx 0.005025
\]
These are the prior odds in favor of carrying HIV.  The prior odds against carrying HIV are
\[
\frac{P(H^c)}{P(H)} = \frac{0.995}{0.005} = 199
\]
That is, prior to taking the test, an American is 199 times more likely to not carry HIV than to carry HIV.
1. The posterior probability of carrying HIV given a positive test is $P(H|E)=0.062$ and the posterior probability of not carrying HIV given a positive test is $P(H^c|E) = 1-0.062 = 0.938$.
\[
\frac{P(H|E)}{P(H^c\E)} = \frac{0.062}{0.938} \approx 0.066
\]
These are the posterior odds in favor of carrying HIV given a positive test.  The posterior odds against carrying HIV given a positive test are
\[
\frac{P(H^c|E)}{P(H|E)} = \frac{0.938}{0.062} \approx 15.1
\]
That is, given a positive test, an American is 15.1 times more likely to not carry HIV than to carry HIV.
1. Comparing the prior and posterior odds in favor of carrying HIV,
\[
BF = \frac{\text{posterior odds}}{\text{prior odds}} = \frac{0.066}{0.005025} = 13.2
\]
The *odds* of carrying HIV are 13.2 times greater given a positive test result than prior to taking the test.  The Bayes Factor is $BF = 13.2$.
1. By definition
\[
BF = \frac{\text{posterior odds}}{\text{prior odds}}
\]
Rearranging yields
\[
\text{posterior odds} = \text{prior odds}\times BF
\]
1. The likelihood of testing positive given HIV is $P(E|H) = 0.977$ and the likelihood of testing positive given no HIV is $P(E|H^c) = 1-0.926 = 0.074$.
\[
 \frac{P(E|H)}{P(E|H^c)} = \frac{0.977}{0.074} = 13.2
\]
This value is the Bayes factor!  So we could have computed the Bayes factor without first computing the posterior probabilities or odds.

```






- If $P(H)$ is the prior probability of $H$, the prior odds (in favor) of $H$ are $P(H)/P(H^c)$
- If $P(H|E)$ is the posterior probability of $H$ given $E$, the posterior odds (in favor) of $H$ given $E$ are $P(H|E)/P(H^c|E)$ 
- The **Bayes factor (BF)** is defined to be the ratio of the posterior odds to the prior odds
\[
BF = \frac{\text{posterior odds}}{\text{prior odds}} = \frac{P(H|E)/P(H^c|E)}{P(H)/P(H^c)}
\]
- The odds form of Bayes rule says
\begin{align*}
\text{posterior odds} & = \text{prior odds} \times \text{Bayes factor}\\
\frac{P(H|E)}{P(H^c|E)} & = \frac{P(H)}{P(H^c)} \times BF
\end{align*} 
- Apply Bayes rule to $P(H|E)$ and $P(H^c|E)$
\begin{align*}
\frac{P(H|E)}{P(H^c|E)} & = \frac{P(E|H)P(H)/P(E)}{P(E|H^c)P(H^c)/P(E)}\\
& =  \frac{P(H)}{P(H^c)} \times \frac{P(E|H)}{P(E|H^c)}\\
\text{posterior odds} & = \text{prior odds} \times \frac{P(E|H)}{P(E|H^c)}
\end{align*} 
- Therefore, the Bayes factor for hypothesis $H$ given evidence $E$ can be calculated as the *ratio of the likelihoods*
\[
BF = \frac{P(E|H)}{P(E|H^c)}
\]
- That is, the Bayes factor can be computed without first computing posterior probabilities or odds.
- **Odds form of Bayes rule**
\begin{align*}
\frac{P(H|E)}{P(H^c|E)} & =  \frac{P(H)}{P(H^c)} \times \frac{P(E|H)}{P(E|H^c)}
\\
\text{posterior odds} & = \text{prior odds} \times \text{Bayes factor}
\end{align*} 


```{example, bayes-false-positive-odds2}

Continuing Example \@ref(exm:bayes-false-positive).  Now suppose that 5% of individuals in a high-risk group carry the HIV virus. Consider a randomly selectd person from this group who takes the test.  Suppose the sensitivity and specificity of the test are the same as in Example \@ref(exm:bayes-false-positive).

```


1. Compute and interpret the prior odds that a person carries HIV.
1. Use the odds form of Bayes rule to compute the posterior odds that the person carries HIV given a positive test, and interpret the posterior odds.
1. Use the posterior odds to compute the posterior probability that the person carries HIV given a positive test.



```{solution, bayes-false-positive-odds2-sol}

to Example \@ref(exm:bayes-false-positive-odds2)

```

1. $P(H)/P(H^c) = 0.05/0.95 = 1/19 \approx 0.0526$. A person in this group is 19 times more likely to not carry HIV than to carry HIV.
1. The posterior odds are the product of the prior odds and the Bayes factor.  The Bayes factor is the ration of the likelihoods. Since the sensitivity and specificity are the same as in the previous example, the likelihoods are the same, and the Bayes factor is the same.
\[
 \frac{P(E|H)}{P(E|H^c)} = \frac{0.977}{0.074} = 13.2
\]
Therefore 
\[
\text{posterior odds} = \text{prior odds} \times \text{Bayes factor} = \frac{1}{19} \times 13.2 \approx \frac{1}{1.44} \approx 0.695
\]
Given a positive test, a person in this group is 1.44 times more likely to not carry HIV than to carry HIV.
1. The odds is the ratios of the posterior probabilities, and we basically just rescale so they add to 1.  The posterior probability is
\[
P(H|E) = \frac{0.695}{1 + 0.695} = \frac{1}{1 + 1.44} \approx 0.410
\]
The Bayes table is below; we have added a row for the ratios to illustrate the odds calculations.

```{r, echo = FALSE}

hypothesis = c("Carries HIV", "Does not carry HIV")

prior = c(0.05, 1 - 0.05)

likelihood = c(0.977, 1 - 0.926) # given positive test

product = prior * likelihood

posterior = product / sum(product)

bayes_table = data.frame(hypothesis,
                         prior,
                         likelihood,
                         product,
                         posterior) %>%
  add_row(hypothesis = "sum",
          prior = sum(prior),
          likelihood = NA,
          product = sum(product),
          posterior = sum(posterior)) %>%
  add_row(hypothesis = "ratio",
          prior = prior[1] / prior[2],
          likelihood = likelihood[1] / likelihood[2],
          product = product[1] / product[2],
          posterior = posterior[1] / posterior[2])

kable(bayes_table, digits = 4, align = 'r')

```

```{example, kissing-intro}

Most people are right-handed, and even the right eye is dominant for most people.
In a [2003 study reported in *Nature*](http://www.nature.com/news/2003/030213/full/news030210-7.html), a German bio-psychologist conjectured that this preference for the right side manifests itself in other ways as well.
In particular, he investigated if people have a tendency to lean their heads to the right when kissing.
The researcher observed kissing couples in public places and recorded whether the couple leaned their heads to the right or left.
(We'll assume this represents a randomly representative selected sample of kissing couples.)
	
The parameter of interest in this study is the population proportion of kissing couples who lean their heads to the right. Denote this unknown parameter $\theta$.  For now we'll only consider two potential values for $\theta$: 1/2 or 2/3.  We could write this as a pair of competing hypotheses.
\begin{align*}
  H_1 & = \{\theta = 1/2\}\\
  H_2 & = \{\theta = 2/3\}
\end{align*}
```

1. Let $Y$ be the number of couples in a random sample of $n$ kissing couples that lean their heads to the right.  What is the distribution of $Y$?  Identify it by name and its relevant parameters.
1. Suppose that the researcher observed 12 kissing couples, 8 of whom leaned their heads to the right (a proportion of 8/12=0.667).  Compute the relevant likelihoods and the corresponding Bayes factor. 
1. Suppose that our prior belief is that the two hypotheses are equally likely.  Determine the posterior probabilities for the two hypotheses.
1. Repeat the previous part but with a prior probability of 0.9 for $H_1$.
1. The full study actually used a sample of 124 kissing couples, of which 80 leaned their heads to the right (a proportion of 80/124 = 0.645).  Compute the relevant likelihoods and the corresponding Bayes factor.
1. Suppose that our prior belief is that the two hypotheses are equally likely.  Determine the posterior probabilities for the two hypotheses given the data from the sample of 124 couples.
1. Repeat the previous part but with a prior probability of 0.9 for $H_1$.
1. Compare the results of the two samples ($n=12$ versus $n=124$).  What do you observe about the influence of the prior?

```{solution, kissing-intro-sol}

to Example \@ref(exm:kissing-intro)

```


1. $Y$, the number of couples in a random sample of $n$ kissing couples that lean their heads to the right, has a Binomial distribution with parameters $n$ and $\theta$.  The probability that $y$ couples in the sample lean right is
\[
P(Y = y) = \binom{n}{y}\theta^y(1-\theta)^{n-y}, \quad y = 0, 1, 2, \ldots, n
\]
which can be computed with `dbinom(y, n, theta)` in R.
1. The evidence is the event of observing 8 couples leaning to the right in a sample of 12, that is, $E=\{Y=8\}$ where $Y$ has a Binomial(12, $\theta$) distribution. If $H_1$ is true, $Y$ has a Binomial(12, 1/2), so the likelihood is
\[
P(E|H_1) = P(Y = 8|\theta = 1/2) = \binom{12}{8}(1/2)^8(1-1/2)^{12-8} = 0.121,
\]
which is `dbinom(8, 12, 1/2)` in R. If $H_2$ is true, $Y$ has a Binomial(12, 2/3) distribution, so the likelihood is
\[
P(E|H_2) = P(Y = 8|\theta = 2/3) = \binom{12}{8}(2/3)^8(1-2/3)^{12-8} = 0.238,
\]
which is `dbinom(8, 12, 2/3)` in R.  The Bayes factor is
\[
BF = \frac{P(E|H_1)}{P(E|H_2)} = \frac{0.121}{0.238} = 0.506
\]
Observing 8 couples leaning right in a sample of 12 kissing couples is about 2 times more likely if $\theta=2/3$ ($H_2$) than if $\theta = 1/2$ ($H_1$).
1. If the prior probabilities are equal, then the posterior probabilities will be in proportion to the likelihoods.  So the posterior probability of $H_2$ will be about 2 times greater than the posterior probability of $H_1$.  In terms of odds: the prior odds of $H_1$ are 0.5/0.5 = 1, so the posterior odds of $H_1$ given $E$ are $1\times 0.506$.  The Bayes table with the posterior probabilities is below.

    ```{r, echo = FALSE}
    
    theta = c(1 / 2, 2 / 3)
    
    thetac= as.character(round(theta, 3))
    
    prior = c(0.5, 1 - 0.5)
    
    n = 12
    
    y = 8
    
    likelihood = dbinom(y, n, theta) # given positive test
    
    product = prior * likelihood
    
    posterior = product / sum(product)
    
    bayes_table = data.frame(theta = thetac,
                             prior,
                             likelihood,
                             product,
                             posterior) %>%
      add_row(theta = "sum",
              prior = sum(prior),
              likelihood = NA,
              product = sum(product),
              posterior = sum(posterior)) %>%
      add_row(theta = "ratio",
              prior = prior[1] / prior[2],
              likelihood = likelihood[1] / likelihood[2],
              product = product[1] / product[2],
              posterior = posterior[1] / posterior[2])
    
    kable(bayes_table, digits = 4, align = 'r')
    
    ```

1. Now the prior odds of $H_1$ are 0.9/0.1 = 9; the prior probability of $H_1$ is 9 times greater than the prior probability of $H_2$. The posterior odds given $E$ are $9\times 0.506 = 4.56$; the posterior probability of $H_1$ is 4.56 times greater than the posterior probability of $H_2$.  Even though observing 8 out of 12 couples leaning right is more likely if $\theta=2/3$ ($H_1$) than if $\theta = 1/2$ ($H_2$), the posterior probability of $H_1$ is greater than the posterior probability of $H_2$ because of the large discrepancy in the prior probabilities.

    ```{r, echo = FALSE}
    
    theta = c(1 / 2, 2 / 3)
    
    thetac= as.character(round(theta, 3))
    
    prior = c(0.9, 1 - 0.9)
    
    n = 12
    
    y = 8
    
    likelihood = dbinom(y, n, theta) # given positive test
    
    product = prior * likelihood
    
    posterior = product / sum(product)
    
    bayes_table = data.frame(theta = thetac,
                             prior,
                             likelihood,
                             product,
                             posterior) %>%
      add_row(theta = "sum",
              prior = sum(prior),
              likelihood = NA,
              product = sum(product),
              posterior = sum(posterior)) %>%
      add_row(theta = "ratio",
              prior = prior[1] / prior[2],
              likelihood = likelihood[1] / likelihood[2],
              product = product[1] / product[2],
              posterior = posterior[1] / posterior[2])
    
    kable(bayes_table, digits = 4, align = 'r')
    
    ```

1. Now the evidence is the event of observing 80 couples leaning to the right in a sample of 124, that is, $E=\{Y=80\}$ where $Y$ has a Binomial(124, $\theta$) distribution. If $H_1$ is true, $Y$ has a Binomial(124, 1/2) distribution, so the likelihood is
\[
P(E|H_1) = P(Y = 80|\theta = 1/2) = \binom{124}{80}(1/2)^{80}(1-1/2)^{124-80} = 0.00037,
\]
which is `dbinom(80, 124, 1/2)` in R. If $H_2$ is true, $Y$ has a Binomial(124, 2/3) distribution, so the likelihood is
\[
P(E|H_2) = P(Y = 80|\theta = 2/3) = \binom{124}{80}(2/3)^{80}(1-2/3)^{124-80} = 0.0658,
\]
which is `dbinom(8, 12, 2/3)` in R.  The Bayes factor is
\[
BF = \frac{P(E|H_1)}{P(E|H_2)} = \frac{0.00037}{0.0657} \approx 0.00566 \approx \frac{1}{176.64}
\]
Observing 80 couples leaning right in a sample of 124 kissing couples is about 177 times more likely if $\theta=2/3$ ($H_2$) than if $\theta = 1/2$ ($H_1$).
1. If the prior probabilities are equal, then the posterior probabilities will be in proportion to the likelihoods.  So the posterior probability of $H_2$ will be about 177 times greater than the posterior probability of $H_1$.  In terms of odds: the prior odds of $H_1$ are 0.5/0.5 = 1, so the posterior odds of $H_1$ given $E$ are $1\times 176.64$.  The Bayes table with the posterior probabilities is below.

    ```{r, echo = FALSE}
    
    theta = c(1 / 2, 2 / 3)
    
    thetac= as.character(round(theta, 3))
    
    prior = c(0.5, 1 - 0.5)
    
    n = 124
    
    y = 80
    
    likelihood = dbinom(y, n, theta) # given positive test
    
    product = prior * likelihood
    
    posterior = product / sum(product)
    
    bayes_table = data.frame(theta = thetac,
                             prior,
                             likelihood,
                             product,
                             posterior) %>%
      add_row(theta = "sum",
              prior = sum(prior),
              likelihood = NA,
              product = sum(product),
              posterior = sum(posterior)) %>%
      add_row(theta = "ratio",
              prior = prior[1] / prior[2],
              likelihood = likelihood[1] / likelihood[2],
              product = product[1] / product[2],
              posterior = posterior[1] / posterior[2])
    
    kable(bayes_table, digits = 4, align = 'r')
    
    ```
    
1. Now the prior odds of $H_1$ are 09/0.1 = 9; the prior probability of $H_1$ is 9 times greater than the prior probability of $H_2$. The posterior odds given $E$ are $9\times (1/176.64) = 1/19.63$; the posterior probability of $H_2$ is 19.63 times greater than the posterior probability of $H_1$.  Even though our prior probability for $H_1$ was very large, the likelihood of the data is so small under $H_1$ compared with $H_2$ that the posterior probability for $H_1$ is small.

    ```{r, echo = FALSE}
    
    theta = c(1 / 2, 2 / 3)
    
    thetac= as.character(round(theta, 3))
    
    prior = c(0.9, 1 - 0.9)
    
    n = 124
    
    y = 80
    
    likelihood = dbinom(y, n, theta) # given positive test
    
    product = prior * likelihood
    
    posterior = product / sum(product)
    
    bayes_table = data.frame(theta = thetac,
                             prior,
                             likelihood,
                             product,
                             posterior) %>%
      add_row(theta = "sum",
              prior = sum(prior),
              likelihood = NA,
              product = sum(product),
              posterior = sum(posterior)) %>%
      add_row(theta = "ratio",
              prior = prior[1] / prior[2],
              likelihood = likelihood[1] / likelihood[2],
              product = product[1] / product[2],
              posterior = posterior[1] / posterior[2])
    
    kable(bayes_table, digits = 4, align = 'r')
    
    ```

1. The prior had much more influence with the smaller sample size.  When the sample size was large, the data, represented by the likelihoods, had much more weight in determining the posterior probabilities.
